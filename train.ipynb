{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from layer import QRNNLayer\n",
    "from model import QRNNModel\n",
    "\n",
    "import data.data_utils as data_utils\n",
    "from data.data_iterator import BiTextIterator\n",
    "from data.data_iterator import prepare_batch\n",
    "from data.data_iterator import prepare_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loading parameters\n",
    "src_vocab='data/train.en.json'\n",
    "tgt_vocab='data/train.de.json'\n",
    "# src_train='data/train.clean.en'\n",
    "# tgt_train='data/train.clean.fr'\n",
    "# src_valid='data/train.clean.en'\n",
    "# tgt_valid='data/train.clean.fr'\n",
    "src_train='data/test.en'\n",
    "tgt_train='data/test.de'\n",
    "src_valid='data/test.en'\n",
    "tgt_valid='data/test.de'\n",
    "\n",
    "# Network parameters\n",
    "kernel_size = 2\n",
    "hidden_size = 10\n",
    "num_layers = 2\n",
    "emb_size = 500\n",
    "num_enc_symbols = 30000\n",
    "num_dec_symbols = 30000\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.0002\n",
    "max_grad_norm = 1.0\n",
    "batch_size = 128\n",
    "max_epochs = 10000\n",
    "maxi_batches = 20\n",
    "max_seq_len = 50\n",
    "display_freq = 100\n",
    "save_freq = 100\n",
    "valid_freq = 100\n",
    "model_dir = 'model/'\n",
    "model_name = 'model.pkl'\n",
    "shuffle = True\n",
    "sort_by_len = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def create_model():\n",
    "    print 'Creating new model parameters..'\n",
    "    model = QRNNModel(QRNNLayer, num_layers, kernel_size,\n",
    "    \t              hidden_size, emb_size, \n",
    "    \t              num_enc_symbols, num_dec_symbols)\n",
    "\n",
    "    # Initialize a model state\n",
    "#    model_state = vars(config)\n",
    "    model_state = {}\n",
    "    model_state['epoch'], model_state['train_steps'] = 0, 0\n",
    "    model_state['state_dict'] = None\n",
    "    \n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    if os.path.exists(model_path):\n",
    "        print 'Reloading model parameters..'\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        model_state['epoch'] = checkpoint['epoch']\n",
    "        model_state['train_steps'] = checkpoint['train_steps']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model, model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "Loading validation data..\n",
      "Creating new model parameters..\n",
      "Reloading model parameters..\n",
      "Using gpu..\n",
      "Training..\n",
      "Epoch 800 DONE\n",
      "Epoch 801 DONE\n",
      "Epoch 802 DONE\n",
      "Epoch 803 DONE\n",
      "Epoch 804 DONE\n",
      "Epoch 805 DONE\n",
      "Epoch 806 DONE\n",
      "Epoch 807 DONE\n",
      "Epoch 808 DONE\n",
      "Epoch 809 DONE\n",
      "Epoch 810 DONE\n",
      "Epoch 811 DONE\n",
      "Epoch 812 DONE\n",
      "Epoch 813 DONE\n",
      "Epoch 814 DONE\n",
      "Epoch 815 DONE\n",
      "Epoch 816 DONE\n",
      "Epoch 817 DONE\n",
      "Epoch 818 DONE\n",
      "Epoch 819 DONE\n",
      "Epoch 820 DONE\n",
      "Epoch 821 DONE\n",
      "Epoch 822 DONE\n",
      "Epoch 823 DONE\n",
      "Epoch 824 DONE\n",
      "Epoch 825 DONE\n",
      "Epoch 826 DONE\n",
      "Epoch 827 DONE\n",
      "Epoch 828 DONE\n",
      "Epoch 829 DONE\n",
      "Epoch 830 DONE\n",
      "Epoch 831 DONE\n",
      "Epoch 832 DONE\n",
      "Epoch 833 DONE\n",
      "Epoch 834 DONE\n",
      "Epoch 835 DONE\n",
      "Epoch 836 DONE\n",
      "Epoch 837 DONE\n",
      "Epoch 838 DONE\n",
      "Epoch 839 DONE\n",
      "Epoch 840 DONE\n",
      "Epoch 841 DONE\n",
      "Epoch 842 DONE\n",
      "Epoch 843 DONE\n",
      "Epoch 844 DONE\n",
      "Epoch 845 DONE\n",
      "Epoch 846 DONE\n",
      "Epoch 847 DONE\n",
      "Epoch 848 DONE\n",
      "Epoch 849 DONE\n",
      "Epoch 850 DONE\n",
      "Epoch 851 DONE\n",
      "Epoch 852 DONE\n",
      "Epoch 853 DONE\n",
      "Epoch 854 DONE\n",
      "Epoch 855 DONE\n",
      "Epoch 856 DONE\n",
      "Epoch 857 DONE\n",
      "Epoch 858 DONE\n",
      "Epoch 859 DONE\n",
      "Epoch 860 DONE\n",
      "Epoch 861 DONE\n",
      "Epoch 862 DONE\n",
      "Epoch 863 DONE\n",
      "Epoch 864 DONE\n",
      "Epoch 865 DONE\n",
      "Epoch 866 DONE\n",
      "Epoch 867 DONE\n",
      "Epoch 868 DONE\n",
      "Epoch 869 DONE\n",
      "Epoch 870 DONE\n",
      "Epoch 871 DONE\n",
      "Epoch 872 DONE\n",
      "Epoch 873 DONE\n",
      "Epoch 874 DONE\n",
      "Epoch 875 DONE\n",
      "Epoch 876 DONE\n",
      "Epoch 877 DONE\n",
      "Epoch 878 DONE\n",
      "Epoch 879 DONE\n",
      "Epoch 880 DONE\n",
      "Epoch 881 DONE\n",
      "Epoch 882 DONE\n",
      "Epoch 883 DONE\n",
      "Epoch 884 DONE\n",
      "Epoch 885 DONE\n",
      "Epoch 886 DONE\n",
      "Epoch 887 DONE\n",
      "Epoch 888 DONE\n",
      "Epoch 889 DONE\n",
      "Epoch 890 DONE\n",
      "Epoch 891 DONE\n",
      "Epoch 892 DONE\n",
      "Epoch 893 DONE\n",
      "Epoch 894 DONE\n",
      "Epoch 895 DONE\n",
      "Epoch 896 DONE\n",
      "Epoch 897 DONE\n",
      "Epoch 898 DONE\n",
      "Epoch  898 Step  900 Perplexity 22.54 Step-time 0.11 133.77 sents/s 2300.80 words/s\n",
      "Validation step\n",
      "validation step loss 18.6620886585\n",
      "  15 samples seen\n",
      "Valid perplexity: 18.66\n",
      "Saving the model..\n",
      "Epoch 899 DONE\n",
      "Epoch 900 DONE\n",
      "Epoch 901 DONE\n",
      "Epoch 902 DONE\n",
      "Epoch 903 DONE\n",
      "Epoch 904 DONE\n",
      "Epoch 905 DONE\n",
      "Epoch 906 DONE\n",
      "Epoch 907 DONE\n",
      "Epoch 908 DONE\n",
      "Epoch 909 DONE\n",
      "Epoch 910 DONE\n",
      "Epoch 911 DONE\n",
      "Epoch 912 DONE\n",
      "Epoch 913 DONE\n",
      "Epoch 914 DONE\n",
      "Epoch 915 DONE\n",
      "Epoch 916 DONE\n",
      "Epoch 917 DONE\n",
      "Epoch 918 DONE\n",
      "Epoch 919 DONE\n",
      "Epoch 920 DONE\n",
      "Epoch 921 DONE\n",
      "Epoch 922 DONE\n",
      "Epoch 923 DONE\n",
      "Epoch 924 DONE\n",
      "Epoch 925 DONE\n",
      "Epoch 926 DONE\n",
      "Epoch 927 DONE\n",
      "Epoch 928 DONE\n",
      "Epoch 929 DONE\n",
      "Epoch 930 DONE\n",
      "Epoch 931 DONE\n",
      "Epoch 932 DONE\n",
      "Epoch 933 DONE\n",
      "Epoch 934 DONE\n",
      "Epoch 935 DONE\n",
      "Epoch 936 DONE\n",
      "Epoch 937 DONE\n",
      "Epoch 938 DONE\n",
      "Epoch 939 DONE\n",
      "Epoch 940 DONE\n",
      "Epoch 941 DONE\n",
      "Epoch 942 DONE\n",
      "Epoch 943 DONE\n",
      "Epoch 944 DONE\n",
      "Epoch 945 DONE\n",
      "Epoch 946 DONE\n",
      "Epoch 947 DONE\n",
      "Epoch 948 DONE\n",
      "Epoch 949 DONE\n",
      "Epoch 950 DONE\n",
      "Epoch 951 DONE\n",
      "Epoch 952 DONE\n",
      "Epoch 953 DONE\n",
      "Epoch 954 DONE\n",
      "Epoch 955 DONE\n",
      "Epoch 956 DONE\n",
      "Epoch 957 DONE\n",
      "Epoch 958 DONE\n",
      "Epoch 959 DONE\n",
      "Epoch 960 DONE\n",
      "Epoch 961 DONE\n",
      "Epoch 962 DONE\n",
      "Epoch 963 DONE\n",
      "Epoch 964 DONE\n",
      "Epoch 965 DONE\n",
      "Epoch 966 DONE\n",
      "Epoch 967 DONE\n",
      "Epoch 968 DONE\n",
      "Epoch 969 DONE\n",
      "Epoch 970 DONE\n",
      "Epoch 971 DONE\n",
      "Epoch 972 DONE\n",
      "Epoch 973 DONE\n",
      "Epoch 974 DONE\n",
      "Epoch 975 DONE\n",
      "Epoch 976 DONE\n",
      "Epoch 977 DONE\n",
      "Epoch 978 DONE\n",
      "Epoch 979 DONE\n",
      "Epoch 980 DONE\n",
      "Epoch 981 DONE\n",
      "Epoch 982 DONE\n",
      "Epoch 983 DONE\n",
      "Epoch 984 DONE\n",
      "Epoch 985 DONE\n",
      "Epoch 986 DONE\n",
      "Epoch 987 DONE\n",
      "Epoch 988 DONE\n",
      "Epoch 989 DONE\n",
      "Epoch 990 DONE\n",
      "Epoch 991 DONE\n",
      "Epoch 992 DONE\n",
      "Epoch 993 DONE\n",
      "Epoch 994 DONE\n",
      "Epoch 995 DONE\n",
      "Epoch 996 DONE\n",
      "Epoch 997 DONE\n",
      "Epoch 998 DONE\n",
      "Epoch  998 Step  1000 Perplexity 15.60 Step-time 0.11 137.78 sents/s 2369.74 words/s\n",
      "Validation step\n",
      "validation step loss 13.0047450266\n",
      "  15 samples seen\n",
      "Valid perplexity: 13.00\n",
      "Saving the model..\n",
      "Epoch 999 DONE\n",
      "Epoch 1000 DONE\n",
      "Epoch 1001 DONE\n",
      "Epoch 1002 DONE\n",
      "Epoch 1003 DONE\n",
      "Epoch 1004 DONE\n",
      "Epoch 1005 DONE\n",
      "Epoch 1006 DONE\n",
      "Epoch 1007 DONE\n",
      "Epoch 1008 DONE\n",
      "Epoch 1009 DONE\n",
      "Epoch 1010 DONE\n",
      "Epoch 1011 DONE\n",
      "Epoch 1012 DONE\n",
      "Epoch 1013 DONE\n",
      "Epoch 1014 DONE\n",
      "Epoch 1015 DONE\n",
      "Epoch 1016 DONE\n",
      "Epoch 1017 DONE\n",
      "Epoch 1018 DONE\n",
      "Epoch 1019 DONE\n",
      "Epoch 1020 DONE\n",
      "Epoch 1021 DONE\n",
      "Epoch 1022 DONE\n",
      "Epoch 1023 DONE\n",
      "Epoch 1024 DONE\n",
      "Epoch 1025 DONE\n",
      "Epoch 1026 DONE\n",
      "Epoch 1027 DONE\n",
      "Epoch 1028 DONE\n",
      "Epoch 1029 DONE\n",
      "Epoch 1030 DONE\n",
      "Epoch 1031 DONE\n",
      "Epoch 1032 DONE\n",
      "Epoch 1033 DONE\n",
      "Epoch 1034 DONE\n",
      "Epoch 1035 DONE\n",
      "Epoch 1036 DONE\n",
      "Epoch 1037 DONE\n",
      "Epoch 1038 DONE\n",
      "Epoch 1039 DONE\n",
      "Epoch 1040 DONE\n",
      "Epoch 1041 DONE\n",
      "Epoch 1042 DONE\n",
      "Epoch 1043 DONE\n",
      "Epoch 1044 DONE\n",
      "Epoch 1045 DONE\n",
      "Epoch 1046 DONE\n",
      "Epoch 1047 DONE\n",
      "Epoch 1048 DONE\n",
      "Epoch 1049 DONE\n",
      "Epoch 1050 DONE\n",
      "Epoch 1051 DONE\n",
      "Epoch 1052 DONE\n",
      "Epoch 1053 DONE\n",
      "Epoch 1054 DONE\n",
      "Epoch 1055 DONE\n",
      "Epoch 1056 DONE\n",
      "Epoch 1057 DONE\n",
      "Epoch 1058 DONE\n",
      "Epoch 1059 DONE\n",
      "Epoch 1060 DONE\n",
      "Epoch 1061 DONE\n",
      "Epoch 1062 DONE\n",
      "Epoch 1063 DONE\n",
      "Epoch 1064 DONE\n",
      "Epoch 1065 DONE\n",
      "Epoch 1066 DONE\n",
      "Epoch 1067 DONE\n",
      "Epoch 1068 DONE\n",
      "Epoch 1069 DONE\n",
      "Epoch 1070 DONE\n",
      "Epoch 1071 DONE\n",
      "Epoch 1072 DONE\n",
      "Epoch 1073 DONE\n",
      "Epoch 1074 DONE\n",
      "Epoch 1075 DONE\n",
      "Epoch 1076 DONE\n",
      "Epoch 1077 DONE\n",
      "Epoch 1078 DONE\n",
      "Epoch 1079 DONE\n",
      "Epoch 1080 DONE\n",
      "Epoch 1081 DONE\n",
      "Epoch 1082 DONE\n",
      "Epoch 1083 DONE\n",
      "Epoch 1084 DONE\n",
      "Epoch 1085 DONE\n",
      "Epoch 1086 DONE\n",
      "Epoch 1087 DONE\n",
      "Epoch 1088 DONE\n",
      "Epoch 1089 DONE\n",
      "Epoch 1090 DONE\n",
      "Epoch 1091 DONE\n",
      "Epoch 1092 DONE\n",
      "Epoch 1093 DONE\n",
      "Epoch 1094 DONE\n",
      "Epoch 1095 DONE\n",
      "Epoch 1096 DONE\n",
      "Epoch 1097 DONE\n",
      "Epoch 1098 DONE\n",
      "Epoch  1098 Step  1100 Perplexity 10.91 Step-time 0.11 140.91 sents/s 2423.65 words/s\n",
      "Validation step\n",
      "validation step loss 9.148796418\n",
      "  15 samples seen\n",
      "Valid perplexity: 9.15\n",
      "Saving the model..\n",
      "Epoch 1099 DONE\n",
      "Epoch 1100 DONE\n",
      "Epoch 1101 DONE\n",
      "Epoch 1102 DONE\n",
      "Epoch 1103 DONE\n",
      "Epoch 1104 DONE\n",
      "Epoch 1105 DONE\n",
      "Epoch 1106 DONE\n",
      "Epoch 1107 DONE\n",
      "Epoch 1108 DONE\n",
      "Epoch 1109 DONE\n",
      "Epoch 1110 DONE\n",
      "Epoch 1111 DONE\n",
      "Epoch 1112 DONE\n",
      "Epoch 1113 DONE\n",
      "Epoch 1114 DONE\n",
      "Epoch 1115 DONE\n",
      "Epoch 1116 DONE\n",
      "Epoch 1117 DONE\n",
      "Epoch 1118 DONE\n",
      "Epoch 1119 DONE\n",
      "Epoch 1120 DONE\n",
      "Epoch 1121 DONE\n",
      "Epoch 1122 DONE\n",
      "Epoch 1123 DONE\n",
      "Epoch 1124 DONE\n",
      "Epoch 1125 DONE\n",
      "Epoch 1126 DONE\n",
      "Epoch 1127 DONE\n",
      "Epoch 1128 DONE\n",
      "Epoch 1129 DONE\n",
      "Epoch 1130 DONE\n",
      "Epoch 1131 DONE\n",
      "Epoch 1132 DONE\n",
      "Epoch 1133 DONE\n",
      "Epoch 1134 DONE\n",
      "Epoch 1135 DONE\n",
      "Epoch 1136 DONE\n",
      "Epoch 1137 DONE\n",
      "Epoch 1138 DONE\n",
      "Epoch 1139 DONE\n",
      "Epoch 1140 DONE\n",
      "Epoch 1141 DONE\n",
      "Epoch 1142 DONE\n",
      "Epoch 1143 DONE\n",
      "Epoch 1144 DONE\n",
      "Epoch 1145 DONE\n",
      "Epoch 1146 DONE\n",
      "Epoch 1147 DONE\n",
      "Epoch 1148 DONE\n",
      "Epoch 1149 DONE\n",
      "Epoch 1150 DONE\n",
      "Epoch 1151 DONE\n",
      "Epoch 1152 DONE\n",
      "Epoch 1153 DONE\n",
      "Epoch 1154 DONE\n",
      "Epoch 1155 DONE\n",
      "Epoch 1156 DONE\n",
      "Epoch 1157 DONE\n",
      "Epoch 1158 DONE\n",
      "Epoch 1159 DONE\n",
      "Epoch 1160 DONE\n",
      "Epoch 1161 DONE\n",
      "Epoch 1162 DONE\n",
      "Epoch 1163 DONE\n",
      "Epoch 1164 DONE\n",
      "Epoch 1165 DONE\n",
      "Epoch 1166 DONE\n",
      "Epoch 1167 DONE\n",
      "Epoch 1168 DONE\n",
      "Epoch 1169 DONE\n",
      "Epoch 1170 DONE\n",
      "Epoch 1171 DONE\n",
      "Epoch 1172 DONE\n",
      "Epoch 1173 DONE\n",
      "Epoch 1174 DONE\n",
      "Epoch 1175 DONE\n",
      "Epoch 1176 DONE\n",
      "Epoch 1177 DONE\n",
      "Epoch 1178 DONE\n",
      "Epoch 1179 DONE\n",
      "Epoch 1180 DONE\n",
      "Epoch 1181 DONE\n",
      "Epoch 1182 DONE\n",
      "Epoch 1183 DONE\n",
      "Epoch 1184 DONE\n",
      "Epoch 1185 DONE\n",
      "Epoch 1186 DONE\n",
      "Epoch 1187 DONE\n",
      "Epoch 1188 DONE\n",
      "Epoch 1189 DONE\n",
      "Epoch 1190 DONE\n",
      "Epoch 1191 DONE\n",
      "Epoch 1192 DONE\n",
      "Epoch 1193 DONE\n",
      "Epoch 1194 DONE\n",
      "Epoch 1195 DONE\n",
      "Epoch 1196 DONE\n",
      "Epoch 1197 DONE\n",
      "Epoch 1198 DONE\n",
      "Epoch  1198 Step  1200 Perplexity 7.75 Step-time 0.11 139.93 sents/s 2406.72 words/s\n",
      "Validation step\n",
      "validation step loss 6.54913213516\n",
      "  15 samples seen\n",
      "Valid perplexity: 6.55\n",
      "Saving the model..\n",
      "Epoch 1199 DONE\n",
      "Epoch 1200 DONE\n",
      "Epoch 1201 DONE\n",
      "Epoch 1202 DONE\n",
      "Epoch 1203 DONE\n",
      "Epoch 1204 DONE\n",
      "Epoch 1205 DONE\n",
      "Epoch 1206 DONE\n",
      "Epoch 1207 DONE\n",
      "Epoch 1208 DONE\n",
      "Epoch 1209 DONE\n",
      "Epoch 1210 DONE\n",
      "Epoch 1211 DONE\n",
      "Epoch 1212 DONE\n",
      "Epoch 1213 DONE\n",
      "Epoch 1214 DONE\n",
      "Epoch 1215 DONE\n",
      "Epoch 1216 DONE\n",
      "Epoch 1217 DONE\n",
      "Epoch 1218 DONE\n",
      "Epoch 1219 DONE\n",
      "Epoch 1220 DONE\n",
      "Epoch 1221 DONE\n",
      "Epoch 1222 DONE\n",
      "Epoch 1223 DONE\n",
      "Epoch 1224 DONE\n",
      "Epoch 1225 DONE\n",
      "Epoch 1226 DONE\n",
      "Epoch 1227 DONE\n",
      "Epoch 1228 DONE\n",
      "Epoch 1229 DONE\n",
      "Epoch 1230 DONE\n",
      "Epoch 1231 DONE\n",
      "Epoch 1232 DONE\n",
      "Epoch 1233 DONE\n",
      "Epoch 1234 DONE\n",
      "Epoch 1235 DONE\n",
      "Epoch 1236 DONE\n",
      "Epoch 1237 DONE\n",
      "Epoch 1238 DONE\n",
      "Epoch 1239 DONE\n",
      "Epoch 1240 DONE\n",
      "Epoch 1241 DONE\n",
      "Epoch 1242 DONE\n",
      "Epoch 1243 DONE\n",
      "Epoch 1244 DONE\n",
      "Epoch 1245 DONE\n",
      "Epoch 1246 DONE\n",
      "Epoch 1247 DONE\n",
      "Epoch 1248 DONE\n",
      "Epoch 1249 DONE\n",
      "Epoch 1250 DONE\n",
      "Epoch 1251 DONE\n",
      "Epoch 1252 DONE\n",
      "Epoch 1253 DONE\n",
      "Epoch 1254 DONE\n",
      "Epoch 1255 DONE\n",
      "Epoch 1256 DONE\n",
      "Epoch 1257 DONE\n",
      "Epoch 1258 DONE\n",
      "Epoch 1259 DONE\n",
      "Epoch 1260 DONE\n",
      "Epoch 1261 DONE\n",
      "Epoch 1262 DONE\n",
      "Epoch 1263 DONE\n",
      "Epoch 1264 DONE\n",
      "Epoch 1265 DONE\n",
      "Epoch 1266 DONE\n",
      "Epoch 1267 DONE\n",
      "Epoch 1268 DONE\n",
      "Epoch 1269 DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1270 DONE\n",
      "Epoch 1271 DONE\n",
      "Epoch 1272 DONE\n",
      "Epoch 1273 DONE\n",
      "Epoch 1274 DONE\n",
      "Epoch 1275 DONE\n",
      "Epoch 1276 DONE\n",
      "Epoch 1277 DONE\n",
      "Epoch 1278 DONE\n",
      "Epoch 1279 DONE\n",
      "Epoch 1280 DONE\n",
      "Epoch 1281 DONE\n",
      "Epoch 1282 DONE\n",
      "Epoch 1283 DONE\n",
      "Epoch 1284 DONE\n",
      "Epoch 1285 DONE\n",
      "Epoch 1286 DONE\n",
      "Epoch 1287 DONE\n",
      "Epoch 1288 DONE\n",
      "Epoch 1289 DONE\n",
      "Epoch 1290 DONE\n",
      "Epoch 1291 DONE\n",
      "Epoch 1292 DONE\n",
      "Epoch 1293 DONE\n",
      "Epoch 1294 DONE\n",
      "Epoch 1295 DONE\n",
      "Epoch 1296 DONE\n",
      "Epoch 1297 DONE\n",
      "Epoch 1298 DONE\n",
      "Epoch  1298 Step  1300 Perplexity 5.58 Step-time 0.11 140.63 sents/s 2418.76 words/s\n",
      "Validation step\n",
      "validation step loss 4.74394651612\n",
      "  15 samples seen\n",
      "Valid perplexity: 4.74\n",
      "Saving the model..\n",
      "Epoch 1299 DONE\n",
      "Epoch 1300 DONE\n",
      "Epoch 1301 DONE\n",
      "Epoch 1302 DONE\n",
      "Epoch 1303 DONE\n",
      "Epoch 1304 DONE\n",
      "Epoch 1305 DONE\n",
      "Epoch 1306 DONE\n",
      "Epoch 1307 DONE\n",
      "Epoch 1308 DONE\n",
      "Epoch 1309 DONE\n",
      "Epoch 1310 DONE\n",
      "Epoch 1311 DONE\n",
      "Epoch 1312 DONE\n",
      "Epoch 1313 DONE\n",
      "Epoch 1314 DONE\n",
      "Epoch 1315 DONE\n",
      "Epoch 1316 DONE\n",
      "Epoch 1317 DONE\n",
      "Epoch 1318 DONE\n",
      "Epoch 1319 DONE\n",
      "Epoch 1320 DONE\n",
      "Epoch 1321 DONE\n",
      "Epoch 1322 DONE\n",
      "Epoch 1323 DONE\n",
      "Epoch 1324 DONE\n",
      "Epoch 1325 DONE\n",
      "Epoch 1326 DONE\n",
      "Epoch 1327 DONE\n",
      "Epoch 1328 DONE\n",
      "Epoch 1329 DONE\n",
      "Epoch 1330 DONE\n",
      "Epoch 1331 DONE\n",
      "Epoch 1332 DONE\n",
      "Epoch 1333 DONE\n",
      "Epoch 1334 DONE\n",
      "Epoch 1335 DONE\n",
      "Epoch 1336 DONE\n",
      "Epoch 1337 DONE\n",
      "Epoch 1338 DONE\n",
      "Epoch 1339 DONE\n",
      "Epoch 1340 DONE\n",
      "Epoch 1341 DONE\n",
      "Epoch 1342 DONE\n",
      "Epoch 1343 DONE\n",
      "Epoch 1344 DONE\n",
      "Epoch 1345 DONE\n",
      "Epoch 1346 DONE\n",
      "Epoch 1347 DONE\n",
      "Epoch 1348 DONE\n",
      "Epoch 1349 DONE\n",
      "Epoch 1350 DONE\n",
      "Epoch 1351 DONE\n",
      "Epoch 1352 DONE\n",
      "Epoch 1353 DONE\n",
      "Epoch 1354 DONE\n",
      "Epoch 1355 DONE\n",
      "Epoch 1356 DONE\n",
      "Epoch 1357 DONE\n",
      "Epoch 1358 DONE\n",
      "Epoch 1359 DONE\n",
      "Epoch 1360 DONE\n",
      "Epoch 1361 DONE\n",
      "Epoch 1362 DONE\n",
      "Epoch 1363 DONE\n",
      "Epoch 1364 DONE\n",
      "Epoch 1365 DONE\n",
      "Epoch 1366 DONE\n",
      "Epoch 1367 DONE\n",
      "Epoch 1368 DONE\n",
      "Epoch 1369 DONE\n",
      "Epoch 1370 DONE\n",
      "Epoch 1371 DONE\n",
      "Epoch 1372 DONE\n",
      "Epoch 1373 DONE\n",
      "Epoch 1374 DONE\n",
      "Epoch 1375 DONE\n",
      "Epoch 1376 DONE\n",
      "Epoch 1377 DONE\n",
      "Epoch 1378 DONE\n",
      "Epoch 1379 DONE\n",
      "Epoch 1380 DONE\n",
      "Epoch 1381 DONE\n",
      "Epoch 1382 DONE\n",
      "Epoch 1383 DONE\n",
      "Epoch 1384 DONE\n",
      "Epoch 1385 DONE\n",
      "Epoch 1386 DONE\n",
      "Epoch 1387 DONE\n",
      "Epoch 1388 DONE\n",
      "Epoch 1389 DONE\n",
      "Epoch 1390 DONE\n",
      "Epoch 1391 DONE\n",
      "Epoch 1392 DONE\n",
      "Epoch 1393 DONE\n",
      "Epoch 1394 DONE\n",
      "Epoch 1395 DONE\n",
      "Epoch 1396 DONE\n",
      "Epoch 1397 DONE\n",
      "Epoch 1398 DONE\n",
      "Epoch  1398 Step  1400 Perplexity 4.07 Step-time 0.11 137.15 sents/s 2359.02 words/s\n",
      "Validation step\n",
      "validation step loss 3.49551451477\n",
      "  15 samples seen\n",
      "Valid perplexity: 3.50\n",
      "Saving the model..\n",
      "Epoch 1399 DONE\n",
      "Epoch 1400 DONE\n",
      "Epoch 1401 DONE\n",
      "Epoch 1402 DONE\n",
      "Epoch 1403 DONE\n",
      "Epoch 1404 DONE\n",
      "Epoch 1405 DONE\n",
      "Epoch 1406 DONE\n",
      "Epoch 1407 DONE\n",
      "Epoch 1408 DONE\n",
      "Epoch 1409 DONE\n",
      "Epoch 1410 DONE\n",
      "Epoch 1411 DONE\n",
      "Epoch 1412 DONE\n",
      "Epoch 1413 DONE\n",
      "Epoch 1414 DONE\n",
      "Epoch 1415 DONE\n",
      "Epoch 1416 DONE\n",
      "Epoch 1417 DONE\n",
      "Epoch 1418 DONE\n",
      "Epoch 1419 DONE\n",
      "Epoch 1420 DONE\n",
      "Epoch 1421 DONE\n",
      "Epoch 1422 DONE\n",
      "Epoch 1423 DONE\n",
      "Epoch 1424 DONE\n",
      "Epoch 1425 DONE\n",
      "Epoch 1426 DONE\n",
      "Epoch 1427 DONE\n",
      "Epoch 1428 DONE\n",
      "Epoch 1429 DONE\n",
      "Epoch 1430 DONE\n",
      "Epoch 1431 DONE\n",
      "Epoch 1432 DONE\n",
      "Epoch 1433 DONE\n",
      "Epoch 1434 DONE\n",
      "Epoch 1435 DONE\n",
      "Epoch 1436 DONE\n",
      "Epoch 1437 DONE\n",
      "Epoch 1438 DONE\n",
      "Epoch 1439 DONE\n",
      "Epoch 1440 DONE\n",
      "Epoch 1441 DONE\n",
      "Epoch 1442 DONE\n",
      "Epoch 1443 DONE\n",
      "Epoch 1444 DONE\n",
      "Epoch 1445 DONE\n",
      "Epoch 1446 DONE\n",
      "Epoch 1447 DONE\n",
      "Epoch 1448 DONE\n",
      "Epoch 1449 DONE\n",
      "Epoch 1450 DONE\n",
      "Epoch 1451 DONE\n",
      "Epoch 1452 DONE\n",
      "Epoch 1453 DONE\n",
      "Epoch 1454 DONE\n",
      "Epoch 1455 DONE\n",
      "Epoch 1456 DONE\n",
      "Epoch 1457 DONE\n",
      "Epoch 1458 DONE\n",
      "Epoch 1459 DONE\n",
      "Epoch 1460 DONE\n",
      "Epoch 1461 DONE\n",
      "Epoch 1462 DONE\n",
      "Epoch 1463 DONE\n",
      "Epoch 1464 DONE\n",
      "Epoch 1465 DONE\n",
      "Epoch 1466 DONE\n",
      "Epoch 1467 DONE\n",
      "Epoch 1468 DONE\n",
      "Epoch 1469 DONE\n",
      "Epoch 1470 DONE\n",
      "Epoch 1471 DONE\n",
      "Epoch 1472 DONE\n",
      "Epoch 1473 DONE\n",
      "Epoch 1474 DONE\n",
      "Epoch 1475 DONE\n",
      "Epoch 1476 DONE\n",
      "Epoch 1477 DONE\n",
      "Epoch 1478 DONE\n",
      "Epoch 1479 DONE\n",
      "Epoch 1480 DONE\n",
      "Epoch 1481 DONE\n",
      "Epoch 1482 DONE\n",
      "Epoch 1483 DONE\n",
      "Epoch 1484 DONE\n",
      "Epoch 1485 DONE\n",
      "Epoch 1486 DONE\n",
      "Epoch 1487 DONE\n",
      "Epoch 1488 DONE\n",
      "Epoch 1489 DONE\n",
      "Epoch 1490 DONE\n",
      "Epoch 1491 DONE\n",
      "Epoch 1492 DONE\n",
      "Epoch 1493 DONE\n",
      "Epoch 1494 DONE\n",
      "Epoch 1495 DONE\n",
      "Epoch 1496 DONE\n",
      "Epoch 1497 DONE\n",
      "Epoch 1498 DONE\n",
      "Epoch  1498 Step  1500 Perplexity 3.06 Step-time 0.11 140.02 sents/s 2408.28 words/s\n",
      "Validation step\n",
      "validation step loss 2.6870012404\n",
      "  15 samples seen\n",
      "Valid perplexity: 2.69\n",
      "Saving the model..\n",
      "Epoch 1499 DONE\n",
      "Epoch 1500 DONE\n",
      "Epoch 1501 DONE\n",
      "Epoch 1502 DONE\n",
      "Epoch 1503 DONE\n",
      "Epoch 1504 DONE\n",
      "Epoch 1505 DONE\n",
      "Epoch 1506 DONE\n",
      "Epoch 1507 DONE\n",
      "Epoch 1508 DONE\n",
      "Epoch 1509 DONE\n",
      "Epoch 1510 DONE\n",
      "Epoch 1511 DONE\n",
      "Epoch 1512 DONE\n",
      "Epoch 1513 DONE\n",
      "Epoch 1514 DONE\n",
      "Epoch 1515 DONE\n",
      "Epoch 1516 DONE\n",
      "Epoch 1517 DONE\n",
      "Epoch 1518 DONE\n",
      "Epoch 1519 DONE\n",
      "Epoch 1520 DONE\n",
      "Epoch 1521 DONE\n",
      "Epoch 1522 DONE\n",
      "Epoch 1523 DONE\n",
      "Epoch 1524 DONE\n",
      "Epoch 1525 DONE\n",
      "Epoch 1526 DONE\n",
      "Epoch 1527 DONE\n",
      "Epoch 1528 DONE\n",
      "Epoch 1529 DONE\n",
      "Epoch 1530 DONE\n",
      "Epoch 1531 DONE\n",
      "Epoch 1532 DONE\n",
      "Epoch 1533 DONE\n",
      "Epoch 1534 DONE\n",
      "Epoch 1535 DONE\n",
      "Epoch 1536 DONE\n",
      "Epoch 1537 DONE\n",
      "Epoch 1538 DONE\n",
      "Epoch 1539 DONE\n",
      "Epoch 1540 DONE\n",
      "Epoch 1541 DONE\n",
      "Epoch 1542 DONE\n",
      "Epoch 1543 DONE\n",
      "Epoch 1544 DONE\n",
      "Epoch 1545 DONE\n",
      "Epoch 1546 DONE\n",
      "Epoch 1547 DONE\n",
      "Epoch 1548 DONE\n",
      "Epoch 1549 DONE\n",
      "Epoch 1550 DONE\n",
      "Epoch 1551 DONE\n",
      "Epoch 1552 DONE\n",
      "Epoch 1553 DONE\n",
      "Epoch 1554 DONE\n",
      "Epoch 1555 DONE\n",
      "Epoch 1556 DONE\n",
      "Epoch 1557 DONE\n",
      "Epoch 1558 DONE\n",
      "Epoch 1559 DONE\n",
      "Epoch 1560 DONE\n",
      "Epoch 1561 DONE\n",
      "Epoch 1562 DONE\n",
      "Epoch 1563 DONE\n",
      "Epoch 1564 DONE\n",
      "Epoch 1565 DONE\n",
      "Epoch 1566 DONE\n",
      "Epoch 1567 DONE\n",
      "Epoch 1568 DONE\n",
      "Epoch 1569 DONE\n",
      "Epoch 1570 DONE\n",
      "Epoch 1571 DONE\n",
      "Epoch 1572 DONE\n",
      "Epoch 1573 DONE\n",
      "Epoch 1574 DONE\n",
      "Epoch 1575 DONE\n",
      "Epoch 1576 DONE\n",
      "Epoch 1577 DONE\n",
      "Epoch 1578 DONE\n",
      "Epoch 1579 DONE\n",
      "Epoch 1580 DONE\n",
      "Epoch 1581 DONE\n",
      "Epoch 1582 DONE\n",
      "Epoch 1583 DONE\n",
      "Epoch 1584 DONE\n",
      "Epoch 1585 DONE\n",
      "Epoch 1586 DONE\n",
      "Epoch 1587 DONE\n",
      "Epoch 1588 DONE\n",
      "Epoch 1589 DONE\n",
      "Epoch 1590 DONE\n",
      "Epoch 1591 DONE\n",
      "Epoch 1592 DONE\n",
      "Epoch 1593 DONE\n",
      "Epoch 1594 DONE\n",
      "Epoch 1595 DONE\n",
      "Epoch 1596 DONE\n",
      "Epoch 1597 DONE\n",
      "Epoch 1598 DONE\n",
      "Epoch  1598 Step  1600 Perplexity 2.41 Step-time 0.11 138.29 sents/s 2378.67 words/s\n",
      "Validation step\n",
      "validation step loss 2.16499364499\n",
      "  15 samples seen\n",
      "Valid perplexity: 2.16\n",
      "Saving the model..\n",
      "Epoch 1599 DONE\n",
      "Epoch 1600 DONE\n",
      "Epoch 1601 DONE\n",
      "Epoch 1602 DONE\n",
      "Epoch 1603 DONE\n",
      "Epoch 1604 DONE\n",
      "Epoch 1605 DONE\n",
      "Epoch 1606 DONE\n",
      "Epoch 1607 DONE\n",
      "Epoch 1608 DONE\n",
      "Epoch 1609 DONE\n",
      "Epoch 1610 DONE\n",
      "Epoch 1611 DONE\n",
      "Epoch 1612 DONE\n",
      "Epoch 1613 DONE\n",
      "Epoch 1614 DONE\n",
      "Epoch 1615 DONE\n",
      "Epoch 1616 DONE\n",
      "Epoch 1617 DONE\n",
      "Epoch 1618 DONE\n",
      "Epoch 1619 DONE\n",
      "Epoch 1620 DONE\n",
      "Epoch 1621 DONE\n",
      "Epoch 1622 DONE\n",
      "Epoch 1623 DONE\n",
      "Epoch 1624 DONE\n",
      "Epoch 1625 DONE\n",
      "Epoch 1626 DONE\n",
      "Epoch 1627 DONE\n",
      "Epoch 1628 DONE\n",
      "Epoch 1629 DONE\n",
      "Epoch 1630 DONE\n",
      "Epoch 1631 DONE\n",
      "Epoch 1632 DONE\n",
      "Epoch 1633 DONE\n",
      "Epoch 1634 DONE\n",
      "Epoch 1635 DONE\n",
      "Epoch 1636 DONE\n",
      "Epoch 1637 DONE\n",
      "Epoch 1638 DONE\n",
      "Epoch 1639 DONE\n",
      "Epoch 1640 DONE\n",
      "Epoch 1641 DONE\n",
      "Epoch 1642 DONE\n",
      "Epoch 1643 DONE\n",
      "Epoch 1644 DONE\n",
      "Epoch 1645 DONE\n",
      "Epoch 1646 DONE\n",
      "Epoch 1647 DONE\n",
      "Epoch 1648 DONE\n",
      "Epoch 1649 DONE\n",
      "Epoch 1650 DONE\n",
      "Epoch 1651 DONE\n",
      "Epoch 1652 DONE\n",
      "Epoch 1653 DONE\n",
      "Epoch 1654 DONE\n",
      "Epoch 1655 DONE\n",
      "Epoch 1656 DONE\n",
      "Epoch 1657 DONE\n",
      "Epoch 1658 DONE\n",
      "Epoch 1659 DONE\n",
      "Epoch 1660 DONE\n",
      "Epoch 1661 DONE\n",
      "Epoch 1662 DONE\n",
      "Epoch 1663 DONE\n",
      "Epoch 1664 DONE\n",
      "Epoch 1665 DONE\n",
      "Epoch 1666 DONE\n",
      "Epoch 1667 DONE\n",
      "Epoch 1668 DONE\n",
      "Epoch 1669 DONE\n",
      "Epoch 1670 DONE\n",
      "Epoch 1671 DONE\n",
      "Epoch 1672 DONE\n",
      "Epoch 1673 DONE\n",
      "Epoch 1674 DONE\n",
      "Epoch 1675 DONE\n",
      "Epoch 1676 DONE\n",
      "Epoch 1677 DONE\n",
      "Epoch 1678 DONE\n",
      "Epoch 1679 DONE\n",
      "Epoch 1680 DONE\n",
      "Epoch 1681 DONE\n",
      "Epoch 1682 DONE\n",
      "Epoch 1683 DONE\n",
      "Epoch 1684 DONE\n",
      "Epoch 1685 DONE\n",
      "Epoch 1686 DONE\n",
      "Epoch 1687 DONE\n",
      "Epoch 1688 DONE\n",
      "Epoch 1689 DONE\n",
      "Epoch 1690 DONE\n",
      "Epoch 1691 DONE\n",
      "Epoch 1692 DONE\n",
      "Epoch 1693 DONE\n",
      "Epoch 1694 DONE\n",
      "Epoch 1695 DONE\n",
      "Epoch 1696 DONE\n",
      "Epoch 1697 DONE\n",
      "Epoch 1698 DONE\n",
      "Epoch  1698 Step  1700 Perplexity 1.98 Step-time 0.10 148.87 sents/s 2560.52 words/s\n",
      "Validation step\n",
      "validation step loss 1.81272465825\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.81\n",
      "Saving the model..\n",
      "Epoch 1699 DONE\n",
      "Epoch 1700 DONE\n",
      "Epoch 1701 DONE\n",
      "Epoch 1702 DONE\n",
      "Epoch 1703 DONE\n",
      "Epoch 1704 DONE\n",
      "Epoch 1705 DONE\n",
      "Epoch 1706 DONE\n",
      "Epoch 1707 DONE\n",
      "Epoch 1708 DONE\n",
      "Epoch 1709 DONE\n",
      "Epoch 1710 DONE\n",
      "Epoch 1711 DONE\n",
      "Epoch 1712 DONE\n",
      "Epoch 1713 DONE\n",
      "Epoch 1714 DONE\n",
      "Epoch 1715 DONE\n",
      "Epoch 1716 DONE\n",
      "Epoch 1717 DONE\n",
      "Epoch 1718 DONE\n",
      "Epoch 1719 DONE\n",
      "Epoch 1720 DONE\n",
      "Epoch 1721 DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1722 DONE\n",
      "Epoch 1723 DONE\n",
      "Epoch 1724 DONE\n",
      "Epoch 1725 DONE\n",
      "Epoch 1726 DONE\n",
      "Epoch 1727 DONE\n",
      "Epoch 1728 DONE\n",
      "Epoch 1729 DONE\n",
      "Epoch 1730 DONE\n",
      "Epoch 1731 DONE\n",
      "Epoch 1732 DONE\n",
      "Epoch 1733 DONE\n",
      "Epoch 1734 DONE\n",
      "Epoch 1735 DONE\n",
      "Epoch 1736 DONE\n",
      "Epoch 1737 DONE\n",
      "Epoch 1738 DONE\n",
      "Epoch 1739 DONE\n",
      "Epoch 1740 DONE\n",
      "Epoch 1741 DONE\n",
      "Epoch 1742 DONE\n",
      "Epoch 1743 DONE\n",
      "Epoch 1744 DONE\n",
      "Epoch 1745 DONE\n",
      "Epoch 1746 DONE\n",
      "Epoch 1747 DONE\n",
      "Epoch 1748 DONE\n",
      "Epoch 1749 DONE\n",
      "Epoch 1750 DONE\n",
      "Epoch 1751 DONE\n",
      "Epoch 1752 DONE\n",
      "Epoch 1753 DONE\n",
      "Epoch 1754 DONE\n",
      "Epoch 1755 DONE\n",
      "Epoch 1756 DONE\n",
      "Epoch 1757 DONE\n",
      "Epoch 1758 DONE\n",
      "Epoch 1759 DONE\n",
      "Epoch 1760 DONE\n",
      "Epoch 1761 DONE\n",
      "Epoch 1762 DONE\n",
      "Epoch 1763 DONE\n",
      "Epoch 1764 DONE\n",
      "Epoch 1765 DONE\n",
      "Epoch 1766 DONE\n",
      "Epoch 1767 DONE\n",
      "Epoch 1768 DONE\n",
      "Epoch 1769 DONE\n",
      "Epoch 1770 DONE\n",
      "Epoch 1771 DONE\n",
      "Epoch 1772 DONE\n",
      "Epoch 1773 DONE\n",
      "Epoch 1774 DONE\n",
      "Epoch 1775 DONE\n",
      "Epoch 1776 DONE\n",
      "Epoch 1777 DONE\n",
      "Epoch 1778 DONE\n",
      "Epoch 1779 DONE\n",
      "Epoch 1780 DONE\n",
      "Epoch 1781 DONE\n",
      "Epoch 1782 DONE\n",
      "Epoch 1783 DONE\n",
      "Epoch 1784 DONE\n",
      "Epoch 1785 DONE\n",
      "Epoch 1786 DONE\n",
      "Epoch 1787 DONE\n",
      "Epoch 1788 DONE\n",
      "Epoch 1789 DONE\n",
      "Epoch 1790 DONE\n",
      "Epoch 1791 DONE\n",
      "Epoch 1792 DONE\n",
      "Epoch 1793 DONE\n",
      "Epoch 1794 DONE\n",
      "Epoch 1795 DONE\n",
      "Epoch 1796 DONE\n",
      "Epoch 1797 DONE\n",
      "Epoch 1798 DONE\n",
      "Epoch  1798 Step  1800 Perplexity 1.69 Step-time 0.11 141.16 sents/s 2427.94 words/s\n",
      "Validation step\n",
      "validation step loss 1.57771940588\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.58\n",
      "Saving the model..\n",
      "Epoch 1799 DONE\n",
      "Epoch 1800 DONE\n",
      "Epoch 1801 DONE\n",
      "Epoch 1802 DONE\n",
      "Epoch 1803 DONE\n",
      "Epoch 1804 DONE\n",
      "Epoch 1805 DONE\n",
      "Epoch 1806 DONE\n",
      "Epoch 1807 DONE\n",
      "Epoch 1808 DONE\n",
      "Epoch 1809 DONE\n",
      "Epoch 1810 DONE\n",
      "Epoch 1811 DONE\n",
      "Epoch 1812 DONE\n",
      "Epoch 1813 DONE\n",
      "Epoch 1814 DONE\n",
      "Epoch 1815 DONE\n",
      "Epoch 1816 DONE\n",
      "Epoch 1817 DONE\n",
      "Epoch 1818 DONE\n",
      "Epoch 1819 DONE\n",
      "Epoch 1820 DONE\n",
      "Epoch 1821 DONE\n",
      "Epoch 1822 DONE\n",
      "Epoch 1823 DONE\n",
      "Epoch 1824 DONE\n",
      "Epoch 1825 DONE\n",
      "Epoch 1826 DONE\n",
      "Epoch 1827 DONE\n",
      "Epoch 1828 DONE\n",
      "Epoch 1829 DONE\n",
      "Epoch 1830 DONE\n",
      "Epoch 1831 DONE\n",
      "Epoch 1832 DONE\n",
      "Epoch 1833 DONE\n",
      "Epoch 1834 DONE\n",
      "Epoch 1835 DONE\n",
      "Epoch 1836 DONE\n",
      "Epoch 1837 DONE\n",
      "Epoch 1838 DONE\n",
      "Epoch 1839 DONE\n",
      "Epoch 1840 DONE\n",
      "Epoch 1841 DONE\n",
      "Epoch 1842 DONE\n",
      "Epoch 1843 DONE\n",
      "Epoch 1844 DONE\n",
      "Epoch 1845 DONE\n",
      "Epoch 1846 DONE\n",
      "Epoch 1847 DONE\n",
      "Epoch 1848 DONE\n",
      "Epoch 1849 DONE\n",
      "Epoch 1850 DONE\n",
      "Epoch 1851 DONE\n",
      "Epoch 1852 DONE\n",
      "Epoch 1853 DONE\n",
      "Epoch 1854 DONE\n",
      "Epoch 1855 DONE\n",
      "Epoch 1856 DONE\n",
      "Epoch 1857 DONE\n",
      "Epoch 1858 DONE\n",
      "Epoch 1859 DONE\n",
      "Epoch 1860 DONE\n",
      "Epoch 1861 DONE\n",
      "Epoch 1862 DONE\n",
      "Epoch 1863 DONE\n",
      "Epoch 1864 DONE\n",
      "Epoch 1865 DONE\n",
      "Epoch 1866 DONE\n",
      "Epoch 1867 DONE\n",
      "Epoch 1868 DONE\n",
      "Epoch 1869 DONE\n",
      "Epoch 1870 DONE\n",
      "Epoch 1871 DONE\n",
      "Epoch 1872 DONE\n",
      "Epoch 1873 DONE\n",
      "Epoch 1874 DONE\n",
      "Epoch 1875 DONE\n",
      "Epoch 1876 DONE\n",
      "Epoch 1877 DONE\n",
      "Epoch 1878 DONE\n",
      "Epoch 1879 DONE\n",
      "Epoch 1880 DONE\n",
      "Epoch 1881 DONE\n",
      "Epoch 1882 DONE\n",
      "Epoch 1883 DONE\n",
      "Epoch 1884 DONE\n",
      "Epoch 1885 DONE\n",
      "Epoch 1886 DONE\n",
      "Epoch 1887 DONE\n",
      "Epoch 1888 DONE\n",
      "Epoch 1889 DONE\n",
      "Epoch 1890 DONE\n",
      "Epoch 1891 DONE\n",
      "Epoch 1892 DONE\n",
      "Epoch 1893 DONE\n",
      "Epoch 1894 DONE\n",
      "Epoch 1895 DONE\n",
      "Epoch 1896 DONE\n",
      "Epoch 1897 DONE\n",
      "Epoch 1898 DONE\n",
      "Epoch  1898 Step  1900 Perplexity 1.49 Step-time 0.11 137.95 sents/s 2372.70 words/s\n",
      "Validation step\n",
      "validation step loss 1.42009576529\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.42\n",
      "Saving the model..\n",
      "Epoch 1899 DONE\n",
      "Epoch 1900 DONE\n",
      "Epoch 1901 DONE\n",
      "Epoch 1902 DONE\n",
      "Epoch 1903 DONE\n",
      "Epoch 1904 DONE\n",
      "Epoch 1905 DONE\n",
      "Epoch 1906 DONE\n",
      "Epoch 1907 DONE\n",
      "Epoch 1908 DONE\n",
      "Epoch 1909 DONE\n",
      "Epoch 1910 DONE\n",
      "Epoch 1911 DONE\n",
      "Epoch 1912 DONE\n",
      "Epoch 1913 DONE\n",
      "Epoch 1914 DONE\n",
      "Epoch 1915 DONE\n",
      "Epoch 1916 DONE\n",
      "Epoch 1917 DONE\n",
      "Epoch 1918 DONE\n",
      "Epoch 1919 DONE\n",
      "Epoch 1920 DONE\n",
      "Epoch 1921 DONE\n",
      "Epoch 1922 DONE\n",
      "Epoch 1923 DONE\n",
      "Epoch 1924 DONE\n",
      "Epoch 1925 DONE\n",
      "Epoch 1926 DONE\n",
      "Epoch 1927 DONE\n",
      "Epoch 1928 DONE\n",
      "Epoch 1929 DONE\n",
      "Epoch 1930 DONE\n",
      "Epoch 1931 DONE\n",
      "Epoch 1932 DONE\n",
      "Epoch 1933 DONE\n",
      "Epoch 1934 DONE\n",
      "Epoch 1935 DONE\n",
      "Epoch 1936 DONE\n",
      "Epoch 1937 DONE\n",
      "Epoch 1938 DONE\n",
      "Epoch 1939 DONE\n",
      "Epoch 1940 DONE\n",
      "Epoch 1941 DONE\n",
      "Epoch 1942 DONE\n",
      "Epoch 1943 DONE\n",
      "Epoch 1944 DONE\n",
      "Epoch 1945 DONE\n",
      "Epoch 1946 DONE\n",
      "Epoch 1947 DONE\n",
      "Epoch 1948 DONE\n",
      "Epoch 1949 DONE\n",
      "Epoch 1950 DONE\n",
      "Epoch 1951 DONE\n",
      "Epoch 1952 DONE\n",
      "Epoch 1953 DONE\n",
      "Epoch 1954 DONE\n",
      "Epoch 1955 DONE\n",
      "Epoch 1956 DONE\n",
      "Epoch 1957 DONE\n",
      "Epoch 1958 DONE\n",
      "Epoch 1959 DONE\n",
      "Epoch 1960 DONE\n",
      "Epoch 1961 DONE\n",
      "Epoch 1962 DONE\n",
      "Epoch 1963 DONE\n",
      "Epoch 1964 DONE\n",
      "Epoch 1965 DONE\n",
      "Epoch 1966 DONE\n",
      "Epoch 1967 DONE\n",
      "Epoch 1968 DONE\n",
      "Epoch 1969 DONE\n",
      "Epoch 1970 DONE\n",
      "Epoch 1971 DONE\n",
      "Epoch 1972 DONE\n",
      "Epoch 1973 DONE\n",
      "Epoch 1974 DONE\n",
      "Epoch 1975 DONE\n",
      "Epoch 1976 DONE\n",
      "Epoch 1977 DONE\n",
      "Epoch 1978 DONE\n",
      "Epoch 1979 DONE\n",
      "Epoch 1980 DONE\n",
      "Epoch 1981 DONE\n",
      "Epoch 1982 DONE\n",
      "Epoch 1983 DONE\n",
      "Epoch 1984 DONE\n",
      "Epoch 1985 DONE\n",
      "Epoch 1986 DONE\n",
      "Epoch 1987 DONE\n",
      "Epoch 1988 DONE\n",
      "Epoch 1989 DONE\n",
      "Epoch 1990 DONE\n",
      "Epoch 1991 DONE\n",
      "Epoch 1992 DONE\n",
      "Epoch 1993 DONE\n",
      "Epoch 1994 DONE\n",
      "Epoch 1995 DONE\n",
      "Epoch 1996 DONE\n",
      "Epoch 1997 DONE\n",
      "Epoch 1998 DONE\n",
      "Epoch  1998 Step  2000 Perplexity 1.36 Step-time 0.11 142.65 sents/s 2453.60 words/s\n",
      "Validation step\n",
      "validation step loss 1.31211201685\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.31\n",
      "Saving the model..\n",
      "Epoch 1999 DONE\n",
      "Epoch 2000 DONE\n",
      "Epoch 2001 DONE\n",
      "Epoch 2002 DONE\n",
      "Epoch 2003 DONE\n",
      "Epoch 2004 DONE\n",
      "Epoch 2005 DONE\n",
      "Epoch 2006 DONE\n",
      "Epoch 2007 DONE\n",
      "Epoch 2008 DONE\n",
      "Epoch 2009 DONE\n",
      "Epoch 2010 DONE\n",
      "Epoch 2011 DONE\n",
      "Epoch 2012 DONE\n",
      "Epoch 2013 DONE\n",
      "Epoch 2014 DONE\n",
      "Epoch 2015 DONE\n",
      "Epoch 2016 DONE\n",
      "Epoch 2017 DONE\n",
      "Epoch 2018 DONE\n",
      "Epoch 2019 DONE\n",
      "Epoch 2020 DONE\n",
      "Epoch 2021 DONE\n",
      "Epoch 2022 DONE\n",
      "Epoch 2023 DONE\n",
      "Epoch 2024 DONE\n",
      "Epoch 2025 DONE\n",
      "Epoch 2026 DONE\n",
      "Epoch 2027 DONE\n",
      "Epoch 2028 DONE\n",
      "Epoch 2029 DONE\n",
      "Epoch 2030 DONE\n",
      "Epoch 2031 DONE\n",
      "Epoch 2032 DONE\n",
      "Epoch 2033 DONE\n",
      "Epoch 2034 DONE\n",
      "Epoch 2035 DONE\n",
      "Epoch 2036 DONE\n",
      "Epoch 2037 DONE\n",
      "Epoch 2038 DONE\n",
      "Epoch 2039 DONE\n",
      "Epoch 2040 DONE\n",
      "Epoch 2041 DONE\n",
      "Epoch 2042 DONE\n",
      "Epoch 2043 DONE\n",
      "Epoch 2044 DONE\n",
      "Epoch 2045 DONE\n",
      "Epoch 2046 DONE\n",
      "Epoch 2047 DONE\n",
      "Epoch 2048 DONE\n",
      "Epoch 2049 DONE\n",
      "Epoch 2050 DONE\n",
      "Epoch 2051 DONE\n",
      "Epoch 2052 DONE\n",
      "Epoch 2053 DONE\n",
      "Epoch 2054 DONE\n",
      "Epoch 2055 DONE\n",
      "Epoch 2056 DONE\n",
      "Epoch 2057 DONE\n",
      "Epoch 2058 DONE\n",
      "Epoch 2059 DONE\n",
      "Epoch 2060 DONE\n",
      "Epoch 2061 DONE\n",
      "Epoch 2062 DONE\n",
      "Epoch 2063 DONE\n",
      "Epoch 2064 DONE\n",
      "Epoch 2065 DONE\n",
      "Epoch 2066 DONE\n",
      "Epoch 2067 DONE\n",
      "Epoch 2068 DONE\n",
      "Epoch 2069 DONE\n",
      "Epoch 2070 DONE\n",
      "Epoch 2071 DONE\n",
      "Epoch 2072 DONE\n",
      "Epoch 2073 DONE\n",
      "Epoch 2074 DONE\n",
      "Epoch 2075 DONE\n",
      "Epoch 2076 DONE\n",
      "Epoch 2077 DONE\n",
      "Epoch 2078 DONE\n",
      "Epoch 2079 DONE\n",
      "Epoch 2080 DONE\n",
      "Epoch 2081 DONE\n",
      "Epoch 2082 DONE\n",
      "Epoch 2083 DONE\n",
      "Epoch 2084 DONE\n",
      "Epoch 2085 DONE\n",
      "Epoch 2086 DONE\n",
      "Epoch 2087 DONE\n",
      "Epoch 2088 DONE\n",
      "Epoch 2089 DONE\n",
      "Epoch 2090 DONE\n",
      "Epoch 2091 DONE\n",
      "Epoch 2092 DONE\n",
      "Epoch 2093 DONE\n",
      "Epoch 2094 DONE\n",
      "Epoch 2095 DONE\n",
      "Epoch 2096 DONE\n",
      "Epoch 2097 DONE\n",
      "Epoch 2098 DONE\n",
      "Epoch  2098 Step  2100 Perplexity 1.27 Step-time 0.11 140.60 sents/s 2418.39 words/s\n",
      "Validation step\n",
      "validation step loss 1.23836321656\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.24\n",
      "Saving the model..\n",
      "Epoch 2099 DONE\n",
      "Epoch 2100 DONE\n",
      "Epoch 2101 DONE\n",
      "Epoch 2102 DONE\n",
      "Epoch 2103 DONE\n",
      "Epoch 2104 DONE\n",
      "Epoch 2105 DONE\n",
      "Epoch 2106 DONE\n",
      "Epoch 2107 DONE\n",
      "Epoch 2108 DONE\n",
      "Epoch 2109 DONE\n",
      "Epoch 2110 DONE\n",
      "Epoch 2111 DONE\n",
      "Epoch 2112 DONE\n",
      "Epoch 2113 DONE\n",
      "Epoch 2114 DONE\n",
      "Epoch 2115 DONE\n",
      "Epoch 2116 DONE\n",
      "Epoch 2117 DONE\n",
      "Epoch 2118 DONE\n",
      "Epoch 2119 DONE\n",
      "Epoch 2120 DONE\n",
      "Epoch 2121 DONE\n",
      "Epoch 2122 DONE\n",
      "Epoch 2123 DONE\n",
      "Epoch 2124 DONE\n",
      "Epoch 2125 DONE\n",
      "Epoch 2126 DONE\n",
      "Epoch 2127 DONE\n",
      "Epoch 2128 DONE\n",
      "Epoch 2129 DONE\n",
      "Epoch 2130 DONE\n",
      "Epoch 2131 DONE\n",
      "Epoch 2132 DONE\n",
      "Epoch 2133 DONE\n",
      "Epoch 2134 DONE\n",
      "Epoch 2135 DONE\n",
      "Epoch 2136 DONE\n",
      "Epoch 2137 DONE\n",
      "Epoch 2138 DONE\n",
      "Epoch 2139 DONE\n",
      "Epoch 2140 DONE\n",
      "Epoch 2141 DONE\n",
      "Epoch 2142 DONE\n",
      "Epoch 2143 DONE\n",
      "Epoch 2144 DONE\n",
      "Epoch 2145 DONE\n",
      "Epoch 2146 DONE\n",
      "Epoch 2147 DONE\n",
      "Epoch 2148 DONE\n",
      "Epoch 2149 DONE\n",
      "Epoch 2150 DONE\n",
      "Epoch 2151 DONE\n",
      "Epoch 2152 DONE\n",
      "Epoch 2153 DONE\n",
      "Epoch 2154 DONE\n",
      "Epoch 2155 DONE\n",
      "Epoch 2156 DONE\n",
      "Epoch 2157 DONE\n",
      "Epoch 2158 DONE\n",
      "Epoch 2159 DONE\n",
      "Epoch 2160 DONE\n",
      "Epoch 2161 DONE\n",
      "Epoch 2162 DONE\n",
      "Epoch 2163 DONE\n",
      "Epoch 2164 DONE\n",
      "Epoch 2165 DONE\n",
      "Epoch 2166 DONE\n",
      "Epoch 2167 DONE\n",
      "Epoch 2168 DONE\n",
      "Epoch 2169 DONE\n",
      "Epoch 2170 DONE\n",
      "Epoch 2171 DONE\n",
      "Epoch 2172 DONE\n",
      "Epoch 2173 DONE\n",
      "Epoch 2174 DONE\n",
      "Epoch 2175 DONE\n",
      "Epoch 2176 DONE\n",
      "Epoch 2177 DONE\n",
      "Epoch 2178 DONE\n",
      "Epoch 2179 DONE\n",
      "Epoch 2180 DONE\n",
      "Epoch 2181 DONE\n",
      "Epoch 2182 DONE\n",
      "Epoch 2183 DONE\n",
      "Epoch 2184 DONE\n",
      "Epoch 2185 DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2186 DONE\n",
      "Epoch 2187 DONE\n",
      "Epoch 2188 DONE\n",
      "Epoch 2189 DONE\n",
      "Epoch 2190 DONE\n",
      "Epoch 2191 DONE\n",
      "Epoch 2192 DONE\n",
      "Epoch 2193 DONE\n",
      "Epoch 2194 DONE\n",
      "Epoch 2195 DONE\n",
      "Epoch 2196 DONE\n",
      "Epoch 2197 DONE\n",
      "Epoch 2198 DONE\n",
      "Epoch  2198 Step  2200 Perplexity 1.21 Step-time 0.10 144.31 sents/s 2482.21 words/s\n",
      "Validation step\n",
      "validation step loss 1.18624368234\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.19\n",
      "Saving the model..\n",
      "Epoch 2199 DONE\n",
      "Epoch 2200 DONE\n",
      "Epoch 2201 DONE\n",
      "Epoch 2202 DONE\n",
      "Epoch 2203 DONE\n",
      "Epoch 2204 DONE\n",
      "Epoch 2205 DONE\n",
      "Epoch 2206 DONE\n",
      "Epoch 2207 DONE\n",
      "Epoch 2208 DONE\n",
      "Epoch 2209 DONE\n",
      "Epoch 2210 DONE\n",
      "Epoch 2211 DONE\n",
      "Epoch 2212 DONE\n",
      "Epoch 2213 DONE\n",
      "Epoch 2214 DONE\n",
      "Epoch 2215 DONE\n",
      "Epoch 2216 DONE\n",
      "Epoch 2217 DONE\n",
      "Epoch 2218 DONE\n",
      "Epoch 2219 DONE\n",
      "Epoch 2220 DONE\n",
      "Epoch 2221 DONE\n",
      "Epoch 2222 DONE\n",
      "Epoch 2223 DONE\n",
      "Epoch 2224 DONE\n",
      "Epoch 2225 DONE\n",
      "Epoch 2226 DONE\n",
      "Epoch 2227 DONE\n",
      "Epoch 2228 DONE\n",
      "Epoch 2229 DONE\n",
      "Epoch 2230 DONE\n",
      "Epoch 2231 DONE\n",
      "Epoch 2232 DONE\n",
      "Epoch 2233 DONE\n",
      "Epoch 2234 DONE\n",
      "Epoch 2235 DONE\n",
      "Epoch 2236 DONE\n",
      "Epoch 2237 DONE\n",
      "Epoch 2238 DONE\n",
      "Epoch 2239 DONE\n",
      "Epoch 2240 DONE\n",
      "Epoch 2241 DONE\n",
      "Epoch 2242 DONE\n",
      "Epoch 2243 DONE\n",
      "Epoch 2244 DONE\n",
      "Epoch 2245 DONE\n",
      "Epoch 2246 DONE\n",
      "Epoch 2247 DONE\n",
      "Epoch 2248 DONE\n",
      "Epoch 2249 DONE\n",
      "Epoch 2250 DONE\n",
      "Epoch 2251 DONE\n",
      "Epoch 2252 DONE\n",
      "Epoch 2253 DONE\n",
      "Epoch 2254 DONE\n",
      "Epoch 2255 DONE\n",
      "Epoch 2256 DONE\n",
      "Epoch 2257 DONE\n",
      "Epoch 2258 DONE\n",
      "Epoch 2259 DONE\n",
      "Epoch 2260 DONE\n",
      "Epoch 2261 DONE\n",
      "Epoch 2262 DONE\n",
      "Epoch 2263 DONE\n",
      "Epoch 2264 DONE\n",
      "Epoch 2265 DONE\n",
      "Epoch 2266 DONE\n",
      "Epoch 2267 DONE\n",
      "Epoch 2268 DONE\n",
      "Epoch 2269 DONE\n",
      "Epoch 2270 DONE\n",
      "Epoch 2271 DONE\n",
      "Epoch 2272 DONE\n",
      "Epoch 2273 DONE\n",
      "Epoch 2274 DONE\n",
      "Epoch 2275 DONE\n",
      "Epoch 2276 DONE\n",
      "Epoch 2277 DONE\n",
      "Epoch 2278 DONE\n",
      "Epoch 2279 DONE\n",
      "Epoch 2280 DONE\n",
      "Epoch 2281 DONE\n",
      "Epoch 2282 DONE\n",
      "Epoch 2283 DONE\n",
      "Epoch 2284 DONE\n",
      "Epoch 2285 DONE\n",
      "Epoch 2286 DONE\n",
      "Epoch 2287 DONE\n",
      "Epoch 2288 DONE\n",
      "Epoch 2289 DONE\n",
      "Epoch 2290 DONE\n",
      "Epoch 2291 DONE\n",
      "Epoch 2292 DONE\n",
      "Epoch 2293 DONE\n",
      "Epoch 2294 DONE\n",
      "Epoch 2295 DONE\n",
      "Epoch 2296 DONE\n",
      "Epoch 2297 DONE\n",
      "Epoch 2298 DONE\n",
      "Epoch  2298 Step  2300 Perplexity 1.17 Step-time 0.10 149.33 sents/s 2568.52 words/s\n",
      "Validation step\n",
      "validation step loss 1.14806063554\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.15\n",
      "Saving the model..\n",
      "Epoch 2299 DONE\n",
      "Epoch 2300 DONE\n",
      "Epoch 2301 DONE\n",
      "Epoch 2302 DONE\n",
      "Epoch 2303 DONE\n",
      "Epoch 2304 DONE\n",
      "Epoch 2305 DONE\n",
      "Epoch 2306 DONE\n",
      "Epoch 2307 DONE\n",
      "Epoch 2308 DONE\n",
      "Epoch 2309 DONE\n",
      "Epoch 2310 DONE\n",
      "Epoch 2311 DONE\n",
      "Epoch 2312 DONE\n",
      "Epoch 2313 DONE\n",
      "Epoch 2314 DONE\n",
      "Epoch 2315 DONE\n",
      "Epoch 2316 DONE\n",
      "Epoch 2317 DONE\n",
      "Epoch 2318 DONE\n",
      "Epoch 2319 DONE\n",
      "Epoch 2320 DONE\n",
      "Epoch 2321 DONE\n",
      "Epoch 2322 DONE\n",
      "Epoch 2323 DONE\n",
      "Epoch 2324 DONE\n",
      "Epoch 2325 DONE\n",
      "Epoch 2326 DONE\n",
      "Epoch 2327 DONE\n",
      "Epoch 2328 DONE\n",
      "Epoch 2329 DONE\n",
      "Epoch 2330 DONE\n",
      "Epoch 2331 DONE\n",
      "Epoch 2332 DONE\n",
      "Epoch 2333 DONE\n",
      "Epoch 2334 DONE\n",
      "Epoch 2335 DONE\n",
      "Epoch 2336 DONE\n",
      "Epoch 2337 DONE\n",
      "Epoch 2338 DONE\n",
      "Epoch 2339 DONE\n",
      "Epoch 2340 DONE\n",
      "Epoch 2341 DONE\n",
      "Epoch 2342 DONE\n",
      "Epoch 2343 DONE\n",
      "Epoch 2344 DONE\n",
      "Epoch 2345 DONE\n",
      "Epoch 2346 DONE\n",
      "Epoch 2347 DONE\n",
      "Epoch 2348 DONE\n",
      "Epoch 2349 DONE\n",
      "Epoch 2350 DONE\n",
      "Epoch 2351 DONE\n",
      "Epoch 2352 DONE\n",
      "Epoch 2353 DONE\n",
      "Epoch 2354 DONE\n",
      "Epoch 2355 DONE\n",
      "Epoch 2356 DONE\n",
      "Epoch 2357 DONE\n",
      "Epoch 2358 DONE\n",
      "Epoch 2359 DONE\n",
      "Epoch 2360 DONE\n",
      "Epoch 2361 DONE\n",
      "Epoch 2362 DONE\n",
      "Epoch 2363 DONE\n",
      "Epoch 2364 DONE\n",
      "Epoch 2365 DONE\n",
      "Epoch 2366 DONE\n",
      "Epoch 2367 DONE\n",
      "Epoch 2368 DONE\n",
      "Epoch 2369 DONE\n",
      "Epoch 2370 DONE\n",
      "Epoch 2371 DONE\n",
      "Epoch 2372 DONE\n",
      "Epoch 2373 DONE\n",
      "Epoch 2374 DONE\n",
      "Epoch 2375 DONE\n",
      "Epoch 2376 DONE\n",
      "Epoch 2377 DONE\n",
      "Epoch 2378 DONE\n",
      "Epoch 2379 DONE\n",
      "Epoch 2380 DONE\n",
      "Epoch 2381 DONE\n",
      "Epoch 2382 DONE\n",
      "Epoch 2383 DONE\n",
      "Epoch 2384 DONE\n",
      "Epoch 2385 DONE\n",
      "Epoch 2386 DONE\n",
      "Epoch 2387 DONE\n",
      "Epoch 2388 DONE\n",
      "Epoch 2389 DONE\n",
      "Epoch 2390 DONE\n",
      "Epoch 2391 DONE\n",
      "Epoch 2392 DONE\n",
      "Epoch 2393 DONE\n",
      "Epoch 2394 DONE\n",
      "Epoch 2395 DONE\n",
      "Epoch 2396 DONE\n",
      "Epoch 2397 DONE\n",
      "Epoch 2398 DONE\n",
      "Epoch  2398 Step  2400 Perplexity 1.13 Step-time 0.11 140.11 sents/s 2409.95 words/s\n",
      "Validation step\n",
      "validation step loss 1.1197463542\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.12\n",
      "Saving the model..\n",
      "Epoch 2399 DONE\n",
      "Epoch 2400 DONE\n",
      "Epoch 2401 DONE\n",
      "Epoch 2402 DONE\n",
      "Epoch 2403 DONE\n",
      "Epoch 2404 DONE\n",
      "Epoch 2405 DONE\n",
      "Epoch 2406 DONE\n",
      "Epoch 2407 DONE\n",
      "Epoch 2408 DONE\n",
      "Epoch 2409 DONE\n",
      "Epoch 2410 DONE\n",
      "Epoch 2411 DONE\n",
      "Epoch 2412 DONE\n",
      "Epoch 2413 DONE\n",
      "Epoch 2414 DONE\n",
      "Epoch 2415 DONE\n",
      "Epoch 2416 DONE\n",
      "Epoch 2417 DONE\n",
      "Epoch 2418 DONE\n",
      "Epoch 2419 DONE\n",
      "Epoch 2420 DONE\n",
      "Epoch 2421 DONE\n",
      "Epoch 2422 DONE\n",
      "Epoch 2423 DONE\n",
      "Epoch 2424 DONE\n",
      "Epoch 2425 DONE\n",
      "Epoch 2426 DONE\n",
      "Epoch 2427 DONE\n",
      "Epoch 2428 DONE\n",
      "Epoch 2429 DONE\n",
      "Epoch 2430 DONE\n",
      "Epoch 2431 DONE\n",
      "Epoch 2432 DONE\n",
      "Epoch 2433 DONE\n",
      "Epoch 2434 DONE\n",
      "Epoch 2435 DONE\n",
      "Epoch 2436 DONE\n",
      "Epoch 2437 DONE\n",
      "Epoch 2438 DONE\n",
      "Epoch 2439 DONE\n",
      "Epoch 2440 DONE\n",
      "Epoch 2441 DONE\n",
      "Epoch 2442 DONE\n",
      "Epoch 2443 DONE\n",
      "Epoch 2444 DONE\n",
      "Epoch 2445 DONE\n",
      "Epoch 2446 DONE\n",
      "Epoch 2447 DONE\n",
      "Epoch 2448 DONE\n",
      "Epoch 2449 DONE\n",
      "Epoch 2450 DONE\n",
      "Epoch 2451 DONE\n",
      "Epoch 2452 DONE\n",
      "Epoch 2453 DONE\n",
      "Epoch 2454 DONE\n",
      "Epoch 2455 DONE\n",
      "Epoch 2456 DONE\n",
      "Epoch 2457 DONE\n",
      "Epoch 2458 DONE\n",
      "Epoch 2459 DONE\n",
      "Epoch 2460 DONE\n",
      "Epoch 2461 DONE\n",
      "Epoch 2462 DONE\n",
      "Epoch 2463 DONE\n",
      "Epoch 2464 DONE\n",
      "Epoch 2465 DONE\n",
      "Epoch 2466 DONE\n",
      "Epoch 2467 DONE\n",
      "Epoch 2468 DONE\n",
      "Epoch 2469 DONE\n",
      "Epoch 2470 DONE\n",
      "Epoch 2471 DONE\n",
      "Epoch 2472 DONE\n",
      "Epoch 2473 DONE\n",
      "Epoch 2474 DONE\n",
      "Epoch 2475 DONE\n",
      "Epoch 2476 DONE\n",
      "Epoch 2477 DONE\n",
      "Epoch 2478 DONE\n",
      "Epoch 2479 DONE\n",
      "Epoch 2480 DONE\n",
      "Epoch 2481 DONE\n",
      "Epoch 2482 DONE\n",
      "Epoch 2483 DONE\n",
      "Epoch 2484 DONE\n",
      "Epoch 2485 DONE\n",
      "Epoch 2486 DONE\n",
      "Epoch 2487 DONE\n",
      "Epoch 2488 DONE\n",
      "Epoch 2489 DONE\n",
      "Epoch 2490 DONE\n",
      "Epoch 2491 DONE\n",
      "Epoch 2492 DONE\n",
      "Epoch 2493 DONE\n",
      "Epoch 2494 DONE\n",
      "Epoch 2495 DONE\n",
      "Epoch 2496 DONE\n",
      "Epoch 2497 DONE\n",
      "Epoch 2498 DONE\n",
      "Epoch  2498 Step  2500 Perplexity 1.11 Step-time 0.11 138.56 sents/s 2383.19 words/s\n",
      "Validation step\n",
      "validation step loss 1.09832317491\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.10\n",
      "Saving the model..\n",
      "Epoch 2499 DONE\n",
      "Epoch 2500 DONE\n",
      "Epoch 2501 DONE\n",
      "Epoch 2502 DONE\n",
      "Epoch 2503 DONE\n",
      "Epoch 2504 DONE\n",
      "Epoch 2505 DONE\n",
      "Epoch 2506 DONE\n",
      "Epoch 2507 DONE\n",
      "Epoch 2508 DONE\n",
      "Epoch 2509 DONE\n",
      "Epoch 2510 DONE\n",
      "Epoch 2511 DONE\n",
      "Epoch 2512 DONE\n",
      "Epoch 2513 DONE\n",
      "Epoch 2514 DONE\n",
      "Epoch 2515 DONE\n",
      "Epoch 2516 DONE\n",
      "Epoch 2517 DONE\n",
      "Epoch 2518 DONE\n",
      "Epoch 2519 DONE\n",
      "Epoch 2520 DONE\n",
      "Epoch 2521 DONE\n",
      "Epoch 2522 DONE\n",
      "Epoch 2523 DONE\n",
      "Epoch 2524 DONE\n",
      "Epoch 2525 DONE\n",
      "Epoch 2526 DONE\n",
      "Epoch 2527 DONE\n",
      "Epoch 2528 DONE\n",
      "Epoch 2529 DONE\n",
      "Epoch 2530 DONE\n",
      "Epoch 2531 DONE\n",
      "Epoch 2532 DONE\n",
      "Epoch 2533 DONE\n",
      "Epoch 2534 DONE\n",
      "Epoch 2535 DONE\n",
      "Epoch 2536 DONE\n",
      "Epoch 2537 DONE\n",
      "Epoch 2538 DONE\n",
      "Epoch 2539 DONE\n",
      "Epoch 2540 DONE\n",
      "Epoch 2541 DONE\n",
      "Epoch 2542 DONE\n",
      "Epoch 2543 DONE\n",
      "Epoch 2544 DONE\n",
      "Epoch 2545 DONE\n",
      "Epoch 2546 DONE\n",
      "Epoch 2547 DONE\n",
      "Epoch 2548 DONE\n",
      "Epoch 2549 DONE\n",
      "Epoch 2550 DONE\n",
      "Epoch 2551 DONE\n",
      "Epoch 2552 DONE\n",
      "Epoch 2553 DONE\n",
      "Epoch 2554 DONE\n",
      "Epoch 2555 DONE\n",
      "Epoch 2556 DONE\n",
      "Epoch 2557 DONE\n",
      "Epoch 2558 DONE\n",
      "Epoch 2559 DONE\n",
      "Epoch 2560 DONE\n",
      "Epoch 2561 DONE\n",
      "Epoch 2562 DONE\n",
      "Epoch 2563 DONE\n",
      "Epoch 2564 DONE\n",
      "Epoch 2565 DONE\n",
      "Epoch 2566 DONE\n",
      "Epoch 2567 DONE\n",
      "Epoch 2568 DONE\n",
      "Epoch 2569 DONE\n",
      "Epoch 2570 DONE\n",
      "Epoch 2571 DONE\n",
      "Epoch 2572 DONE\n",
      "Epoch 2573 DONE\n",
      "Epoch 2574 DONE\n",
      "Epoch 2575 DONE\n",
      "Epoch 2576 DONE\n",
      "Epoch 2577 DONE\n",
      "Epoch 2578 DONE\n",
      "Epoch 2579 DONE\n",
      "Epoch 2580 DONE\n",
      "Epoch 2581 DONE\n",
      "Epoch 2582 DONE\n",
      "Epoch 2583 DONE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-43ed9ef0aeaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdec_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mstep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mstep_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jay/.conda/envs/jay/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jay/.conda/envs/jay/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load parallel data to train\n",
    "# TODO: using PyTorch DataIterator\n",
    "print 'Loading training data..'\n",
    "train_set = BiTextIterator(source=src_train,\n",
    "                           target=tgt_train,\n",
    "                           source_dict=src_vocab,\n",
    "                           target_dict=tgt_vocab,\n",
    "                           batch_size=batch_size,\n",
    "                           maxlen=max_seq_len,\n",
    "                           n_words_source=num_enc_symbols,\n",
    "                           n_words_target=num_dec_symbols,\n",
    "                           shuffle_each_epoch=shuffle,\n",
    "                           sort_by_length=sort_by_len,\n",
    "                           maxibatch_size=maxi_batches)\n",
    "\n",
    "if src_valid and tgt_valid:\n",
    "    print 'Loading validation data..'\n",
    "    valid_set = BiTextIterator(source=src_valid,\n",
    "                               target=tgt_valid,\n",
    "                               source_dict=src_vocab,\n",
    "                               target_dict=tgt_vocab,\n",
    "                               batch_size=batch_size,\n",
    "                               maxlen=None,\n",
    "                               shuffle_each_epoch=False,\n",
    "                               n_words_source=num_enc_symbols,\n",
    "                               n_words_target=num_dec_symbols)\n",
    "else:\n",
    "    valid_set = None\n",
    "\n",
    "# Create a Quasi-RNN model\n",
    "model, model_state = create_model()\n",
    "if use_cuda:\n",
    "    print 'Using gpu..'\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data_utils.pad_token)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss = 0.0\n",
    "words_seen, sents_seen = 0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "print 'Training..'\n",
    "for epoch_idx in xrange(max_epochs):\n",
    "    if model_state['epoch'] >= max_epochs:\n",
    "        print 'Training is already complete.', \\\n",
    "              'current epoch:{}, max epoch:{}'.format(model_state['epoch'], max_epochs)\n",
    "        break\n",
    "\n",
    "    for source_seq, target_seq in train_set:    \n",
    "        # Get a batch from training parallel data\n",
    "        enc_input, enc_len, dec_input, dec_target, dec_len = \\\n",
    "            prepare_train_batch(source_seq, target_seq, max_seq_len)\n",
    "\n",
    "        if use_cuda:\n",
    "            enc_input = Variable(enc_input.cuda())\n",
    "            enc_len = Variable(enc_len.cuda())\n",
    "            dec_input = Variable(dec_input.cuda())\n",
    "            dec_target = Variable(dec_target.cuda())\n",
    "            dec_len = Variable(dec_len.cuda())\n",
    "        else:\n",
    "            enc_input = Variable(enc_input)\n",
    "            enc_len = Variable(enc_len)\n",
    "            dec_input = Variable(dec_input)\n",
    "            dec_target = Variable(dec_target)\n",
    "            dec_len = Variable(dec_len)\n",
    "\n",
    "        if enc_input is None or dec_input is None or dec_target is None:\n",
    "            print 'No samples under max_seq_length ', max_seq_len\n",
    "            continue\n",
    "\n",
    "        # Execute a single training step\n",
    "        optimizer.zero_grad()\n",
    "        dec_logits = model(enc_input, enc_len, dec_input)\n",
    "        step_loss = criterion(dec_logits, dec_target.view(-1))\n",
    "        step_loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += float(step_loss.data[0]) / display_freq\n",
    "        words_seen += torch.sum(enc_len + dec_len).data[0]\n",
    "        sents_seen += enc_input.size(0)  # batch_size\n",
    "\n",
    "        model_state['train_steps'] += 1\n",
    "\n",
    "        # Display training status\n",
    "        if model_state['train_steps'] % display_freq == 0:\n",
    "\n",
    "            avg_perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "            time_elapsed = time.time() - start_time\n",
    "            step_time = time_elapsed / display_freq\n",
    "\n",
    "            words_per_sec = words_seen / time_elapsed\n",
    "            sents_per_sec = sents_seen / time_elapsed\n",
    "\n",
    "            print 'Epoch ', model_state['epoch'], 'Step ', model_state['train_steps'], \\\n",
    "                  'Perplexity {0:.2f}'.format(avg_perplexity), 'Step-time {0:.2f}'.format(step_time), \\\n",
    "                  '{0:.2f} sents/s'.format(sents_per_sec), '{0:.2f} words/s'.format(words_per_sec)\n",
    "\n",
    "            loss = 0.0\n",
    "            words_seen, sents_seen = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Execute a validation process\n",
    "        if valid_set and model_state['train_steps'] % valid_freq == 0:\n",
    "            print 'Validation step'\n",
    "\n",
    "            valid_steps = 0\n",
    "            valid_loss = 0.0\n",
    "            valid_sents_seen = 0\n",
    "            for source_seq, target_seq in valid_set:\n",
    "                # Get a batch from validation parallel data\n",
    "                enc_input, enc_len, dec_input, dec_target, dec_len = \\\n",
    "                    prepare_train_batch(source_seq, target_seq)\n",
    "\n",
    "                if use_cuda:\n",
    "                    enc_input = Variable(enc_input.cuda())\n",
    "                    enc_len = Variable(enc_len.cuda())\n",
    "                    dec_input = Variable(dec_input.cuda())\n",
    "                    dec_target = Variable(dec_target.cuda())\n",
    "                    dec_len = Variable(dec_len.cuda())\n",
    "                else:\n",
    "                    enc_input = Variable(enc_input)\n",
    "                    enc_len = Variable(enc_len)\n",
    "                    dec_input = Variable(dec_input)\n",
    "                    dec_target = Variable(dec_target)\n",
    "                    dec_len = Variable(dec_len)\n",
    "\n",
    "                dec_logits = model(enc_input, enc_len, dec_input)\n",
    "                step_loss = criterion(dec_logits, dec_target.view(-1))\n",
    "                print 'validation step loss', math.exp(step_loss.data[0])\n",
    "                valid_steps += 1 \n",
    "                valid_loss += float(step_loss.data[0])\n",
    "                valid_sents_seen += enc_input.size(0)\n",
    "                print '  {} samples seen'.format(valid_sents_seen)\n",
    "\n",
    "            print 'Valid perplexity: {0:.2f}'.format(math.exp(valid_loss / valid_steps))\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        if model_state['train_steps'] % save_freq == 0:\n",
    "            print 'Saving the model..'\n",
    "\n",
    "            model_state['state_dict'] = model.state_dict()\n",
    "#                state = dict(list(model_state.items()))\n",
    "            model_path = os.path.join(model_dir, model_name)\n",
    "            torch.save(model_state, model_path)\n",
    "\n",
    "    # Increase the epoch index of the model\n",
    "    model_state['epoch'] += 1\n",
    "    print 'Epoch {0:} DONE'.format(model_state['epoch'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
