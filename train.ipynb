{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from layer import QRNNLayer\n",
    "from model import QRNNModel\n",
    "\n",
    "import data.data_utils as data_utils\n",
    "from data.data_iterator import BiTextIterator\n",
    "from data.data_iterator import prepare_batch\n",
    "from data.data_iterator import prepare_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loading parameters\n",
    "src_vocab='data/train.clean.en.json'\n",
    "tgt_vocab='data/train.clean.fr.json'\n",
    "# src_train='data/train.clean.en'\n",
    "# tgt_train='data/train.clean.fr'\n",
    "# src_valid='data/train.clean.en'\n",
    "# tgt_valid='data/train.clean.fr'\n",
    "src_train='data/test.en'\n",
    "tgt_train='data/test.fr'\n",
    "src_valid='data/test.en'\n",
    "tgt_valid='data/test.fr'\n",
    "\n",
    "# Network parameters\n",
    "kernel_size = 2\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "emb_size = 500\n",
    "num_enc_symbols = 30000\n",
    "num_dec_symbols = 30000\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.0002\n",
    "max_grad_norm = 1.0\n",
    "batch_size = 128\n",
    "max_epochs = 1000\n",
    "maxi_batches = 20\n",
    "max_seq_len = 50\n",
    "display_freq = 100\n",
    "save_freq = 100\n",
    "valid_freq = 100\n",
    "model_dir = 'model/'\n",
    "model_name = 'model.pkl'\n",
    "shuffle = True\n",
    "sort_by_len = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def create_model():\n",
    "    print 'Creating new model parameters..'\n",
    "    model = QRNNModel(QRNNLayer, num_layers, kernel_size,\n",
    "    \t              hidden_size, emb_size, \n",
    "    \t              num_enc_symbols, num_dec_symbols)\n",
    "\n",
    "    # Initialize a model state\n",
    "#    model_state = vars(config)\n",
    "    model_state = {}\n",
    "    model_state['epoch'], model_state['train_steps'] = 0, 0\n",
    "    model_state['state_dict'] = None\n",
    "    \n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    if os.path.exists(model_path):\n",
    "        print 'Reloading model parameters..'\n",
    "        checkpoint = torch.load(model_path)\n",
    "\n",
    "        model_state['epoch'] = checkpoint['epoch']\n",
    "        model_state['train_steps'] = checkpoint['train_steps']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    return model, model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data..\n",
      "Loading validation data..\n",
      "Creating new model parameters..\n",
      "Using gpu..\n",
      "Training..\n",
      "Epoch 1 DONE\n",
      "Epoch 2 DONE\n",
      "Epoch 3 DONE\n",
      "Epoch 4 DONE\n",
      "Epoch 5 DONE\n",
      "Epoch 6 DONE\n",
      "Epoch 7 DONE\n",
      "Epoch 8 DONE\n",
      "Epoch 9 DONE\n",
      "Epoch 10 DONE\n",
      "Epoch 11 DONE\n",
      "Epoch 12 DONE\n",
      "Epoch 13 DONE\n",
      "Epoch 14 DONE\n",
      "Epoch 15 DONE\n",
      "Epoch 16 DONE\n",
      "Epoch 17 DONE\n",
      "Epoch 18 DONE\n",
      "Epoch 19 DONE\n",
      "Epoch 20 DONE\n",
      "Epoch 21 DONE\n",
      "Epoch 22 DONE\n",
      "Epoch 23 DONE\n",
      "Epoch 24 DONE\n",
      "Epoch 25 DONE\n",
      "Epoch 26 DONE\n",
      "Epoch 27 DONE\n",
      "Epoch 28 DONE\n",
      "Epoch 29 DONE\n",
      "Epoch 30 DONE\n",
      "Epoch 31 DONE\n",
      "Epoch 32 DONE\n",
      "Epoch 33 DONE\n",
      "Epoch 34 DONE\n",
      "Epoch 35 DONE\n",
      "Epoch 36 DONE\n",
      "Epoch 37 DONE\n",
      "Epoch 38 DONE\n",
      "Epoch 39 DONE\n",
      "Epoch 40 DONE\n",
      "Epoch 41 DONE\n",
      "Epoch 42 DONE\n",
      "Epoch 43 DONE\n",
      "Epoch 44 DONE\n",
      "Epoch 45 DONE\n",
      "Epoch 46 DONE\n",
      "Epoch 47 DONE\n",
      "Epoch 48 DONE\n",
      "Epoch 49 DONE\n",
      "Epoch 50 DONE\n",
      "Epoch 51 DONE\n",
      "Epoch 52 DONE\n",
      "Epoch 53 DONE\n",
      "Epoch 54 DONE\n",
      "Epoch 55 DONE\n",
      "Epoch 56 DONE\n",
      "Epoch 57 DONE\n",
      "Epoch 58 DONE\n",
      "Epoch 59 DONE\n",
      "Epoch 60 DONE\n",
      "Epoch 61 DONE\n",
      "Epoch 62 DONE\n",
      "Epoch 63 DONE\n",
      "Epoch 64 DONE\n",
      "Epoch 65 DONE\n",
      "Epoch 66 DONE\n",
      "Epoch 67 DONE\n",
      "Epoch 68 DONE\n",
      "Epoch 69 DONE\n",
      "Epoch 70 DONE\n",
      "Epoch 71 DONE\n",
      "Epoch 72 DONE\n",
      "Epoch 73 DONE\n",
      "Epoch 74 DONE\n",
      "Epoch 75 DONE\n",
      "Epoch 76 DONE\n",
      "Epoch 77 DONE\n",
      "Epoch 78 DONE\n",
      "Epoch 79 DONE\n",
      "Epoch 80 DONE\n",
      "Epoch 81 DONE\n",
      "Epoch 82 DONE\n",
      "Epoch 83 DONE\n",
      "Epoch 84 DONE\n",
      "Epoch 85 DONE\n",
      "Epoch 86 DONE\n",
      "Epoch 87 DONE\n",
      "Epoch 88 DONE\n",
      "Epoch 89 DONE\n",
      "Epoch 90 DONE\n",
      "Epoch 91 DONE\n",
      "Epoch 92 DONE\n",
      "Epoch 93 DONE\n",
      "Epoch 94 DONE\n",
      "Epoch 95 DONE\n",
      "Epoch 96 DONE\n",
      "Epoch 97 DONE\n",
      "Epoch 98 DONE\n",
      "Epoch 99 DONE\n",
      "Epoch  99 Step  100 Perplexity 5.14 Step-time 0.11 138.56 sents/s 2226.17 words/s\n",
      "Validation step\n",
      "validation step loss 1.00021681632\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.00\n",
      "Saving the model..\n",
      "Epoch 100 DONE\n",
      "Epoch 101 DONE\n",
      "Epoch 102 DONE\n",
      "Epoch 103 DONE\n",
      "Epoch 104 DONE\n",
      "Epoch 105 DONE\n",
      "Epoch 106 DONE\n",
      "Epoch 107 DONE\n",
      "Epoch 108 DONE\n",
      "Epoch 109 DONE\n",
      "Epoch 110 DONE\n",
      "Epoch 111 DONE\n",
      "Epoch 112 DONE\n",
      "Epoch 113 DONE\n",
      "Epoch 114 DONE\n",
      "Epoch 115 DONE\n",
      "Epoch 116 DONE\n",
      "Epoch 117 DONE\n",
      "Epoch 118 DONE\n",
      "Epoch 119 DONE\n",
      "Epoch 120 DONE\n",
      "Epoch 121 DONE\n",
      "Epoch 122 DONE\n",
      "Epoch 123 DONE\n",
      "Epoch 124 DONE\n",
      "Epoch 125 DONE\n",
      "Epoch 126 DONE\n",
      "Epoch 127 DONE\n",
      "Epoch 128 DONE\n",
      "Epoch 129 DONE\n",
      "Epoch 130 DONE\n",
      "Epoch 131 DONE\n",
      "Epoch 132 DONE\n",
      "Epoch 133 DONE\n",
      "Epoch 134 DONE\n",
      "Epoch 135 DONE\n",
      "Epoch 136 DONE\n",
      "Epoch 137 DONE\n",
      "Epoch 138 DONE\n",
      "Epoch 139 DONE\n",
      "Epoch 140 DONE\n",
      "Epoch 141 DONE\n",
      "Epoch 142 DONE\n",
      "Epoch 143 DONE\n",
      "Epoch 144 DONE\n",
      "Epoch 145 DONE\n",
      "Epoch 146 DONE\n",
      "Epoch 147 DONE\n",
      "Epoch 148 DONE\n",
      "Epoch 149 DONE\n",
      "Epoch 150 DONE\n",
      "Epoch 151 DONE\n",
      "Epoch 152 DONE\n",
      "Epoch 153 DONE\n",
      "Epoch 154 DONE\n",
      "Epoch 155 DONE\n",
      "Epoch 156 DONE\n",
      "Epoch 157 DONE\n",
      "Epoch 158 DONE\n",
      "Epoch 159 DONE\n",
      "Epoch 160 DONE\n",
      "Epoch 161 DONE\n",
      "Epoch 162 DONE\n",
      "Epoch 163 DONE\n",
      "Epoch 164 DONE\n",
      "Epoch 165 DONE\n",
      "Epoch 166 DONE\n",
      "Epoch 167 DONE\n",
      "Epoch 168 DONE\n",
      "Epoch 169 DONE\n",
      "Epoch 170 DONE\n",
      "Epoch 171 DONE\n",
      "Epoch 172 DONE\n",
      "Epoch 173 DONE\n",
      "Epoch 174 DONE\n",
      "Epoch 175 DONE\n",
      "Epoch 176 DONE\n",
      "Epoch 177 DONE\n",
      "Epoch 178 DONE\n",
      "Epoch 179 DONE\n",
      "Epoch 180 DONE\n",
      "Epoch 181 DONE\n",
      "Epoch 182 DONE\n",
      "Epoch 183 DONE\n",
      "Epoch 184 DONE\n",
      "Epoch 185 DONE\n",
      "Epoch 186 DONE\n",
      "Epoch 187 DONE\n",
      "Epoch 188 DONE\n",
      "Epoch 189 DONE\n",
      "Epoch 190 DONE\n",
      "Epoch 191 DONE\n",
      "Epoch 192 DONE\n",
      "Epoch 193 DONE\n",
      "Epoch 194 DONE\n",
      "Epoch 195 DONE\n",
      "Epoch 196 DONE\n",
      "Epoch 197 DONE\n",
      "Epoch 198 DONE\n",
      "Epoch 199 DONE\n",
      "Epoch  199 Step  200 Perplexity 1.00 Step-time 0.11 140.26 sents/s 2253.55 words/s\n",
      "Validation step\n",
      "validation step loss 1.00011974475\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.00\n",
      "Saving the model..\n",
      "Epoch 200 DONE\n",
      "Epoch 201 DONE\n",
      "Epoch 202 DONE\n",
      "Epoch 203 DONE\n",
      "Epoch 204 DONE\n",
      "Epoch 205 DONE\n",
      "Epoch 206 DONE\n",
      "Epoch 207 DONE\n",
      "Epoch 208 DONE\n",
      "Epoch 209 DONE\n",
      "Epoch 210 DONE\n",
      "Epoch 211 DONE\n",
      "Epoch 212 DONE\n",
      "Epoch 213 DONE\n",
      "Epoch 214 DONE\n",
      "Epoch 215 DONE\n",
      "Epoch 216 DONE\n",
      "Epoch 217 DONE\n",
      "Epoch 218 DONE\n",
      "Epoch 219 DONE\n",
      "Epoch 220 DONE\n",
      "Epoch 221 DONE\n",
      "Epoch 222 DONE\n",
      "Epoch 223 DONE\n",
      "Epoch 224 DONE\n",
      "Epoch 225 DONE\n",
      "Epoch 226 DONE\n",
      "Epoch 227 DONE\n",
      "Epoch 228 DONE\n",
      "Epoch 229 DONE\n",
      "Epoch 230 DONE\n",
      "Epoch 231 DONE\n",
      "Epoch 232 DONE\n",
      "Epoch 233 DONE\n",
      "Epoch 234 DONE\n",
      "Epoch 235 DONE\n",
      "Epoch 236 DONE\n",
      "Epoch 237 DONE\n",
      "Epoch 238 DONE\n",
      "Epoch 239 DONE\n",
      "Epoch 240 DONE\n",
      "Epoch 241 DONE\n",
      "Epoch 242 DONE\n",
      "Epoch 243 DONE\n",
      "Epoch 244 DONE\n",
      "Epoch 245 DONE\n",
      "Epoch 246 DONE\n",
      "Epoch 247 DONE\n",
      "Epoch 248 DONE\n",
      "Epoch 249 DONE\n",
      "Epoch 250 DONE\n",
      "Epoch 251 DONE\n",
      "Epoch 252 DONE\n",
      "Epoch 253 DONE\n",
      "Epoch 254 DONE\n",
      "Epoch 255 DONE\n",
      "Epoch 256 DONE\n",
      "Epoch 257 DONE\n",
      "Epoch 258 DONE\n",
      "Epoch 259 DONE\n",
      "Epoch 260 DONE\n",
      "Epoch 261 DONE\n",
      "Epoch 262 DONE\n",
      "Epoch 263 DONE\n",
      "Epoch 264 DONE\n",
      "Epoch 265 DONE\n",
      "Epoch 266 DONE\n",
      "Epoch 267 DONE\n",
      "Epoch 268 DONE\n",
      "Epoch 269 DONE\n",
      "Epoch 270 DONE\n",
      "Epoch 271 DONE\n",
      "Epoch 272 DONE\n",
      "Epoch 273 DONE\n",
      "Epoch 274 DONE\n",
      "Epoch 275 DONE\n",
      "Epoch 276 DONE\n",
      "Epoch 277 DONE\n",
      "Epoch 278 DONE\n",
      "Epoch 279 DONE\n",
      "Epoch 280 DONE\n",
      "Epoch 281 DONE\n",
      "Epoch 282 DONE\n",
      "Epoch 283 DONE\n",
      "Epoch 284 DONE\n",
      "Epoch 285 DONE\n",
      "Epoch 286 DONE\n",
      "Epoch 287 DONE\n",
      "Epoch 288 DONE\n",
      "Epoch 289 DONE\n",
      "Epoch 290 DONE\n",
      "Epoch 291 DONE\n",
      "Epoch 292 DONE\n",
      "Epoch 293 DONE\n",
      "Epoch 294 DONE\n",
      "Epoch 295 DONE\n",
      "Epoch 296 DONE\n",
      "Epoch 297 DONE\n",
      "Epoch 298 DONE\n",
      "Epoch 299 DONE\n",
      "Epoch  299 Step  300 Perplexity 1.00 Step-time 0.14 110.52 sents/s 1775.62 words/s\n",
      "Validation step\n",
      "validation step loss 1.00007860232\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.00\n",
      "Saving the model..\n",
      "Epoch 300 DONE\n",
      "Epoch 301 DONE\n",
      "Epoch 302 DONE\n",
      "Epoch 303 DONE\n",
      "Epoch 304 DONE\n",
      "Epoch 305 DONE\n",
      "Epoch 306 DONE\n",
      "Epoch 307 DONE\n",
      "Epoch 308 DONE\n",
      "Epoch 309 DONE\n",
      "Epoch 310 DONE\n",
      "Epoch 311 DONE\n",
      "Epoch 312 DONE\n",
      "Epoch 313 DONE\n",
      "Epoch 314 DONE\n",
      "Epoch 315 DONE\n",
      "Epoch 316 DONE\n",
      "Epoch 317 DONE\n",
      "Epoch 318 DONE\n",
      "Epoch 319 DONE\n",
      "Epoch 320 DONE\n",
      "Epoch 321 DONE\n",
      "Epoch 322 DONE\n",
      "Epoch 323 DONE\n",
      "Epoch 324 DONE\n",
      "Epoch 325 DONE\n",
      "Epoch 326 DONE\n",
      "Epoch 327 DONE\n",
      "Epoch 328 DONE\n",
      "Epoch 329 DONE\n",
      "Epoch 330 DONE\n",
      "Epoch 331 DONE\n",
      "Epoch 332 DONE\n",
      "Epoch 333 DONE\n",
      "Epoch 334 DONE\n",
      "Epoch 335 DONE\n",
      "Epoch 336 DONE\n",
      "Epoch 337 DONE\n",
      "Epoch 338 DONE\n",
      "Epoch 339 DONE\n",
      "Epoch 340 DONE\n",
      "Epoch 341 DONE\n",
      "Epoch 342 DONE\n",
      "Epoch 343 DONE\n",
      "Epoch 344 DONE\n",
      "Epoch 345 DONE\n",
      "Epoch 346 DONE\n",
      "Epoch 347 DONE\n",
      "Epoch 348 DONE\n",
      "Epoch 349 DONE\n",
      "Epoch 350 DONE\n",
      "Epoch 351 DONE\n",
      "Epoch 352 DONE\n",
      "Epoch 353 DONE\n",
      "Epoch 354 DONE\n",
      "Epoch 355 DONE\n",
      "Epoch 356 DONE\n",
      "Epoch 357 DONE\n",
      "Epoch 358 DONE\n",
      "Epoch 359 DONE\n",
      "Epoch 360 DONE\n",
      "Epoch 361 DONE\n",
      "Epoch 362 DONE\n",
      "Epoch 363 DONE\n",
      "Epoch 364 DONE\n",
      "Epoch 365 DONE\n",
      "Epoch 366 DONE\n",
      "Epoch 367 DONE\n",
      "Epoch 368 DONE\n",
      "Epoch 369 DONE\n",
      "Epoch 370 DONE\n",
      "Epoch 371 DONE\n",
      "Epoch 372 DONE\n",
      "Epoch 373 DONE\n",
      "Epoch 374 DONE\n",
      "Epoch 375 DONE\n",
      "Epoch 376 DONE\n",
      "Epoch 377 DONE\n",
      "Epoch 378 DONE\n",
      "Epoch 379 DONE\n",
      "Epoch 380 DONE\n",
      "Epoch 381 DONE\n",
      "Epoch 382 DONE\n",
      "Epoch 383 DONE\n",
      "Epoch 384 DONE\n",
      "Epoch 385 DONE\n",
      "Epoch 386 DONE\n",
      "Epoch 387 DONE\n",
      "Epoch 388 DONE\n",
      "Epoch 389 DONE\n",
      "Epoch 390 DONE\n",
      "Epoch 391 DONE\n",
      "Epoch 392 DONE\n",
      "Epoch 393 DONE\n",
      "Epoch 394 DONE\n",
      "Epoch 395 DONE\n",
      "Epoch 396 DONE\n",
      "Epoch 397 DONE\n",
      "Epoch 398 DONE\n",
      "Epoch 399 DONE\n",
      "Epoch  399 Step  400 Perplexity 1.00 Step-time 0.11 131.05 sents/s 2105.54 words/s\n",
      "Validation step\n",
      "validation step loss 1.00005594588\n",
      "  15 samples seen\n",
      "Valid perplexity: 1.00\n",
      "Saving the model..\n",
      "Epoch 400 DONE\n",
      "Epoch 401 DONE\n",
      "Epoch 402 DONE\n",
      "Epoch 403 DONE\n",
      "Epoch 404 DONE\n",
      "Epoch 405 DONE\n",
      "Epoch 406 DONE\n",
      "Epoch 407 DONE\n",
      "Epoch 408 DONE\n",
      "Epoch 409 DONE\n",
      "Epoch 410 DONE\n",
      "Epoch 411 DONE\n",
      "Epoch 412 DONE\n",
      "Epoch 413 DONE\n",
      "Epoch 414 DONE\n",
      "Epoch 415 DONE\n",
      "Epoch 416 DONE\n",
      "Epoch 417 DONE\n",
      "Epoch 418 DONE\n",
      "Epoch 419 DONE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-43ed9ef0aeaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Execute a single training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdec_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mstep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mstep_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pjh/ml_tutorials/pytorch/quasi-rnn/model.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_inputs, enc_len, dec_inputs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# logits: [batch_size x length, tgt_vocab_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pjh/ml_tutorials/pytorch/quasi-rnn/model.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, inputs, init_states, memories, keep_len)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         cell_states, hidden_states = self.decoder(inputs, init_states, \n\u001b[0;32m---> 91\u001b[0;31m                                                   memories, keep_len)\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;31m# return:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# projected hidden_state of the last layer: logit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pjh/ml_tutorials/pytorch/quasi-rnn/model.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, init_states, memories, keep_len)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mcell_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pjh/ml_tutorials/pytorch/quasi-rnn/layer.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, state, memory, keep_len)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mc_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rnn_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mc_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mh_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, split_size, dim)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/functional.pyc\u001b[0m in \u001b[0;36msplit\u001b[0;34m(tensor, split_size, dim)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msplit_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_splits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlast_split_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     return tuple(tensor.narrow(int(dim), int(i * split_size), int(get_split_size(i))) for i\n\u001b[0;32m---> 31\u001b[0;31m                  in _range(0, num_splits))\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/functional.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((i,))\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msplit_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_splits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlast_split_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     return tuple(tensor.narrow(int(dim), int(i * split_size), int(get_split_size(i))) for i\n\u001b[0m\u001b[1;32m     31\u001b[0m                  in _range(0, num_splits))\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mnarrow\u001b[0;34m(self, dim, start_index, length)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load parallel data to train\n",
    "# TODO: using PyTorch DataIterator\n",
    "print 'Loading training data..'\n",
    "train_set = BiTextIterator(source=src_train,\n",
    "                           target=tgt_train,\n",
    "                           source_dict=src_vocab,\n",
    "                           target_dict=tgt_vocab,\n",
    "                           batch_size=batch_size,\n",
    "                           maxlen=max_seq_len,\n",
    "                           n_words_source=num_enc_symbols,\n",
    "                           n_words_target=num_dec_symbols,\n",
    "                           shuffle_each_epoch=shuffle,\n",
    "                           sort_by_length=sort_by_len,\n",
    "                           maxibatch_size=maxi_batches)\n",
    "\n",
    "if src_valid and tgt_valid:\n",
    "    print 'Loading validation data..'\n",
    "    valid_set = BiTextIterator(source=src_valid,\n",
    "                               target=tgt_valid,\n",
    "                               source_dict=src_vocab,\n",
    "                               target_dict=tgt_vocab,\n",
    "                               batch_size=batch_size,\n",
    "                               maxlen=None,\n",
    "                               shuffle_each_epoch=False,\n",
    "                               n_words_source=num_enc_symbols,\n",
    "                               n_words_target=num_dec_symbols)\n",
    "else:\n",
    "    valid_set = None\n",
    "\n",
    "# Create a Quasi-RNN model\n",
    "model, model_state = create_model()\n",
    "if use_cuda:\n",
    "    print 'Using gpu..'\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=data_utils.pad_token)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "loss = 0.0\n",
    "words_seen, sents_seen = 0, 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "print 'Training..'\n",
    "for epoch_idx in xrange(max_epochs):\n",
    "    if model_state['epoch'] >= max_epochs:\n",
    "        print 'Training is already complete.', \\\n",
    "              'current epoch:{}, max epoch:{}'.format(model_state['epoch'], max_epochs)\n",
    "        break\n",
    "\n",
    "    for source_seq, target_seq in train_set:    \n",
    "        # Get a batch from training parallel data\n",
    "        enc_input, enc_len, dec_input, dec_target, dec_len = \\\n",
    "            prepare_train_batch(source_seq, target_seq, max_seq_len)\n",
    "\n",
    "        if use_cuda:\n",
    "            enc_input = Variable(enc_input.cuda())\n",
    "            enc_len = Variable(enc_len.cuda())\n",
    "            dec_input = Variable(dec_input.cuda())\n",
    "            dec_target = Variable(dec_target.cuda())\n",
    "            dec_len = Variable(dec_len.cuda())\n",
    "        else:\n",
    "            enc_input = Variable(enc_input)\n",
    "            enc_len = Variable(enc_len)\n",
    "            dec_input = Variable(dec_input)\n",
    "            dec_target = Variable(dec_target)\n",
    "            dec_len = Variable(dec_len)\n",
    "\n",
    "        if enc_input is None or dec_input is None or dec_target is None:\n",
    "            print 'No samples under max_seq_length ', max_seq_len\n",
    "            continue\n",
    "\n",
    "        # Execute a single training step\n",
    "        optimizer.zero_grad()\n",
    "        dec_logits = model(enc_input, enc_len, dec_input)\n",
    "        step_loss = criterion(dec_logits, dec_target.view(-1))\n",
    "        step_loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += float(step_loss.data[0]) / display_freq\n",
    "        words_seen += torch.sum(enc_len + dec_len).data[0]\n",
    "        sents_seen += enc_input.size(0)  # batch_size\n",
    "\n",
    "        model_state['train_steps'] += 1\n",
    "\n",
    "        # Display training status\n",
    "        if model_state['train_steps'] % display_freq == 0:\n",
    "\n",
    "            avg_perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "            time_elapsed = time.time() - start_time\n",
    "            step_time = time_elapsed / display_freq\n",
    "\n",
    "            words_per_sec = words_seen / time_elapsed\n",
    "            sents_per_sec = sents_seen / time_elapsed\n",
    "\n",
    "            print 'Epoch ', model_state['epoch'], 'Step ', model_state['train_steps'], \\\n",
    "                  'Perplexity {0:.2f}'.format(avg_perplexity), 'Step-time {0:.2f}'.format(step_time), \\\n",
    "                  '{0:.2f} sents/s'.format(sents_per_sec), '{0:.2f} words/s'.format(words_per_sec)\n",
    "\n",
    "            loss = 0.0\n",
    "            words_seen, sents_seen = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Execute a validation process\n",
    "        if valid_set and model_state['train_steps'] % valid_freq == 0:\n",
    "            print 'Validation step'\n",
    "\n",
    "            valid_steps = 0\n",
    "            valid_loss = 0.0\n",
    "            valid_sents_seen = 0\n",
    "            for source_seq, target_seq in valid_set:\n",
    "                # Get a batch from validation parallel data\n",
    "                enc_input, enc_len, dec_input, dec_target, dec_len = \\\n",
    "                    prepare_train_batch(source_seq, target_seq)\n",
    "\n",
    "                if use_cuda:\n",
    "                    enc_input = Variable(enc_input.cuda())\n",
    "                    enc_len = Variable(enc_len.cuda())\n",
    "                    dec_input = Variable(dec_input.cuda())\n",
    "                    dec_target = Variable(dec_target.cuda())\n",
    "                    dec_len = Variable(dec_len.cuda())\n",
    "                else:\n",
    "                    enc_input = Variable(enc_input)\n",
    "                    enc_len = Variable(enc_len)\n",
    "                    dec_input = Variable(dec_input)\n",
    "                    dec_target = Variable(dec_target)\n",
    "                    dec_len = Variable(dec_len)\n",
    "\n",
    "                dec_logits = model(enc_input, enc_len, dec_input)\n",
    "                step_loss = criterion(dec_logits, dec_target.view(-1))\n",
    "                print 'validation step loss', math.exp(step_loss.data[0])\n",
    "                valid_steps += 1 \n",
    "                valid_loss += float(step_loss.data[0])\n",
    "                valid_sents_seen += enc_input.size(0)\n",
    "                print '  {} samples seen'.format(valid_sents_seen)\n",
    "\n",
    "            print 'Valid perplexity: {0:.2f}'.format(math.exp(valid_loss / valid_steps))\n",
    "\n",
    "        # Save the model checkpoint\n",
    "        if model_state['train_steps'] % save_freq == 0:\n",
    "            print 'Saving the model..'\n",
    "\n",
    "            model_state['state_dict'] = model.state_dict()\n",
    "#                state = dict(list(model_state.items()))\n",
    "            model_path = os.path.join(model_dir, model_name)\n",
    "            torch.save(model_state, model_path)\n",
    "\n",
    "    # Increase the epoch index of the model\n",
    "    model_state['epoch'] += 1\n",
    "    print 'Epoch {0:} DONE'.format(model_state['epoch'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
